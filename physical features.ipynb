{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b7e53-9061-457d-a1f4-77e4a8abbf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install osmnx geopandas pandas openpyxl shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e198e-a7ee-446b-a057-940185dd78bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box\n",
    "import warnings\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "SEARCH_RADIUS = 5000  # meters = 5KM\n",
    "OUTPUT_FILE = \"Air_Quality_with_OSM_Features_5KM.xlsx\"\n",
    "INPUT_FILE = r\"data/processed/india_aq_transformed_last30days.csv\"\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE EXTRACTION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_roads(lat: float, lon: float, radius: int) -> Dict:\n",
    "    \"\"\"Extract road features from OSM\"\"\"\n",
    "    try:\n",
    "        roads = ox.features_from_point(\n",
    "            (lat, lon),\n",
    "            tags={'highway': True},\n",
    "            dist=radius\n",
    "        )\n",
    "        \n",
    "        if len(roads) > 0:\n",
    "            road_names = [roads.index.get_level_values(0).unique()[i] \n",
    "                         for i in range(min(5, len(roads.index.get_level_values(0).unique())))]\n",
    "            return {\n",
    "                'count': len(roads),\n",
    "                'examples': ', '.join(str(r)[:40] for r in road_names),\n",
    "                'status': 'success'\n",
    "            }\n",
    "        return {'count': 0, 'examples': '', 'status': 'success'}\n",
    "    except Exception as e:\n",
    "        return {'count': 0, 'examples': f\"Error: {str(e)[:30]}\", 'status': 'failed'}\n",
    "\n",
    "def extract_industrial_zones(lat: float, lon: float, radius: int) -> Dict:\n",
    "    \"\"\"Extract industrial facilities from OSM\"\"\"\n",
    "    try:\n",
    "        tags = {'industrial': True, 'landuse': 'industrial'}\n",
    "        industrial = ox.features_from_point((lat, lon), tags=tags, dist=radius)\n",
    "        \n",
    "        if len(industrial) > 0:\n",
    "            ind_names = industrial.index.get_level_values(0).unique()[:5]\n",
    "            return {\n",
    "                'count': len(industrial),\n",
    "                'examples': ', '.join(str(i)[:35] for i in ind_names),\n",
    "                'status': 'success'\n",
    "            }\n",
    "        return {'count': 0, 'examples': '', 'status': 'success'}\n",
    "    except Exception as e:\n",
    "        return {'count': 0, 'examples': f\"Error: {str(e)[:30]}\", 'status': 'failed'}\n",
    "\n",
    "def extract_dump_sites(lat: float, lon: float, radius: int) -> Dict:\n",
    "    \"\"\"Extract waste management and dump sites from OSM\"\"\"\n",
    "    try:\n",
    "        tags = {'waste': True}\n",
    "        dumps = ox.features_from_point((lat, lon), tags=tags, dist=radius)\n",
    "        \n",
    "        if len(dumps) > 0:\n",
    "            dump_names = dumps.index.get_level_values(0).unique()[:5]\n",
    "            return {\n",
    "                'count': len(dumps),\n",
    "                'examples': ', '.join(str(d)[:35] for d in dump_names),\n",
    "                'status': 'success'\n",
    "            }\n",
    "        return {'count': 0, 'examples': '', 'status': 'success'}\n",
    "    except Exception as e:\n",
    "        return {'count': 0, 'examples': f\"Error: {str(e)[:30]}\", 'status': 'failed'}\n",
    "\n",
    "def extract_agricultural_fields(lat: float, lon: float, radius: int) -> Dict:\n",
    "    \"\"\"Extract agricultural areas from OSM\"\"\"\n",
    "    try:\n",
    "        tags = {'landuse': ['farmland', 'farm', 'agricultural', 'grass', 'meadow']}\n",
    "        agriculture = ox.features_from_point((lat, lon), tags=tags, dist=radius)\n",
    "        \n",
    "        if len(agriculture) > 0:\n",
    "            agr_names = agriculture.index.get_level_values(0).unique()[:5]\n",
    "            return {\n",
    "                'count': len(agriculture),\n",
    "                'examples': ', '.join(str(a)[:35] for a in agr_names),\n",
    "                'status': 'success'\n",
    "            }\n",
    "        return {'count': 0, 'examples': '', 'status': 'success'}\n",
    "    except Exception as e:\n",
    "        return {'count': 0, 'examples': f\"Error: {str(e)[:30]}\", 'status': 'failed'}\n",
    "\n",
    "def extract_all_features(lat: float, lon: float, radius: int) -> Dict:\n",
    "    \"\"\"Extract all features for a location\"\"\"\n",
    "    features = {\n",
    "        'roads': extract_roads(lat, lon, radius),\n",
    "        'industrial': extract_industrial_zones(lat, lon, radius),\n",
    "        'dumps': extract_dump_sites(lat, lon, radius),\n",
    "        'agriculture': extract_agricultural_fields(lat, lon, radius)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"OSM FEATURE EXTRACTION FOR AIR QUALITY ANALYSIS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(f\"Loading dataset from {INPUT_FILE}...\")\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"âœ“ Loaded {len(df)} records\\n\")\n",
    "    \n",
    "    # Get unique locations\n",
    "    unique_locations = df[['latitude', 'longitude','location_name', 'state',\n",
    "                           'district']].drop_duplicates(\n",
    "                           subset=['latitude', 'longitude']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Unique locations to process: {len(unique_locations)}\")\n",
    "    print(f\"Processing parameters:\")\n",
    "    print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"  - Search radius: {SEARCH_RADIUS}m\")\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    # Initialize results\n",
    "    features_data = []\n",
    "    total_batches = (len(unique_locations) - 1) // BATCH_SIZE + 1\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_num in range(0, len(unique_locations), BATCH_SIZE):\n",
    "        batch = unique_locations.iloc[batch_num:batch_num + BATCH_SIZE]\n",
    "        batch_idx = (batch_num // BATCH_SIZE) + 1\n",
    "        \n",
    "        print(f\"Batch {batch_idx}/{total_batches}:\")\n",
    "        print(f\"Processing locations {batch_num + 1} to {min(batch_num + BATCH_SIZE, len(unique_locations))}\\n\")\n",
    "        \n",
    "        for idx, row in batch.iterrows():\n",
    "            lat = row['latitude']\n",
    "            lon = row['longitude']\n",
    "            location_name = row['location_name']\n",
    "            city = row['district']\n",
    "            state = row['state']\n",
    "            \n",
    "            try:\n",
    "                # Extract features\n",
    "                print(f\"  Querying {city}, {state} ({lat:.4f}, {lon:.4f})...\", end='', flush=True)\n",
    "                features = extract_all_features(lat, lon, SEARCH_RADIUS)\n",
    "                \n",
    "                # Compile record\n",
    "                record = {\n",
    "                    'location_name': location_name,\n",
    "                    'district': city,\n",
    "                    'state': state,\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'Roads_count': features['roads']['count'],\n",
    "                    'Roads_examples': features['roads']['examples'],\n",
    "                    'Industrial_zones_count': features['industrial']['count'],\n",
    "                    'Industrial_examples': features['industrial']['examples'],\n",
    "                    'Dump_sites_count': features['dumps']['count'],\n",
    "                    'Dump_examples': features['dumps']['examples'],\n",
    "                    'Agricultural_fields_count': features['agriculture']['count'],\n",
    "                    'Agricultural_examples': features['agriculture']['examples'],\n",
    "                    'Query_status': 'Success'\n",
    "                }\n",
    "                \n",
    "                features_data.append(record)\n",
    "                \n",
    "                print(f\" âœ“ Roads:{features['roads']['count']} | \"\n",
    "                      f\"Industrial:{features['industrial']['count']} | \"\n",
    "                      f\"Dumps:{features['dumps']['count']} | \"\n",
    "                      f\"Agriculture:{features['agriculture']['count']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" âœ— Error: {str(e)[:50]}\")\n",
    "                record = {\n",
    "                    'location_name': location_name,\n",
    "                    'district': city,\n",
    "                    'state': state,\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'Roads_count': 0,\n",
    "                    'Roads_examples': '',\n",
    "                    'Industrial_zones_count': 0,\n",
    "                    'Industrial_examples': '',\n",
    "                    'Dump_sites_count': 0,\n",
    "                    'Dump_examples': '',\n",
    "                    'Agricultural_fields_count': 0,\n",
    "                    'Agricultural_examples': '',\n",
    "                    'Query_status': f'Failed: {str(e)[:40]}'\n",
    "                }\n",
    "                features_data.append(record)\n",
    "            \n",
    "            # Small delay between queries to avoid rate limiting\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        print(f\"\\nBatch completed. Waiting before next batch...\\n\")\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Create features dataframe\n",
    "    features_df = pd.DataFrame(features_data)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXTRACTION SUMMARY\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Total locations processed: {len(features_df)}\")\n",
    "    successful = (features_df['Query_status'] == 'Success').sum()\n",
    "    print(f\"Successful queries: {successful}/{len(features_df)}\")\n",
    "    \n",
    "    print(f\"\\n{'Feature Statistics':^80}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"{'Feature':<20} {'Total':<12} {'Mean':<12} {'Max':<12}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"{'Roads':<20} {features_df['Roads_count'].sum():<12.0f} \"\n",
    "          f\"{features_df['Roads_count'].mean():<12.1f} {features_df['Roads_count'].max():<12.0f}\")\n",
    "    print(f\"{'Industrial':<20} {features_df['Industrial_zones_count'].sum():<12.0f} \"\n",
    "          f\"{features_df['Industrial_zones_count'].mean():<12.1f} \"\n",
    "          f\"{features_df['Industrial_zones_count'].max():<12.0f}\")\n",
    "    print(f\"{'Dump Sites':<20} {features_df['Dump_sites_count'].sum():<12.0f} \"\n",
    "          f\"{features_df['Dump_sites_count'].mean():<12.1f} {features_df['Dump_sites_count'].max():<12.0f}\")\n",
    "    print(f\"{'Agriculture':<20} {features_df['Agricultural_fields_count'].sum():<12.0f} \"\n",
    "          f\"{features_df['Agricultural_fields_count'].mean():<12.1f} \"\n",
    "          f\"{features_df['Agricultural_fields_count'].max():<12.0f}\")\n",
    "    print(f\"{'-'*80}\\n\")\n",
    "    \n",
    "    # Merge with original data\n",
    "    print(\"Merging extracted features with original air quality data...\")\n",
    "    \n",
    "    # Merge on latitude and longitude\n",
    "    merged_df = df.merge(\n",
    "        features_df[['latitude', 'longitude', 'Roads_count', 'Industrial_zones_count',\n",
    "                     'Dump_sites_count', 'Agricultural_fields_count', 'Query_status']],\n",
    "        on=['latitude', 'longitude'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill NaN values for unmatched records (should be minimal)\n",
    "    merged_df.fillna({\n",
    "    'Roads_count': 0,\n",
    "    'Industrial_zones_count': 0,\n",
    "    'Dump_sites_count': 0,\n",
    "    'Agricultural_fields_count': 0,\n",
    "    'Query_status': 'Not processed'\n",
    "     }, inplace=True)\n",
    "    \n",
    "    # Create additional derived features\n",
    "    merged_df['Urban_density_score'] = (merged_df['Roads_count'] / merged_df['Roads_count'].max()).round(2)\n",
    "    merged_df['Industrial_presence'] = (merged_df['Industrial_zones_count'] > 0).astype(int)\n",
    "    merged_df['Pollution_source_risk'] = (\n",
    "        (merged_df['Industrial_zones_count'] * 0.4 + \n",
    "         merged_df['Dump_sites_count'] * 0.3 + \n",
    "         merged_df['Roads_count'] * 0.3) / 100\n",
    "    ).round(2)\n",
    "    merged_df['Green_area_ratio'] = (\n",
    "        merged_df['Agricultural_fields_count'] / \n",
    "        (merged_df['Roads_count'] + merged_df['Agricultural_fields_count'])\n",
    "    ).round(2)\n",
    "    \n",
    "    # Save to Excel\n",
    "    print(f\"\\nSaving merged data to {OUTPUT_FILE}...\")\n",
    "    with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:\n",
    "        merged_df.to_excel(writer, sheet_name='All Data', index=False)\n",
    "        features_df.to_excel(writer, sheet_name='OSM Features', index=False)\n",
    "        \n",
    "        # Summary statistics sheet\n",
    "        summary_stats = pd.DataFrame({\n",
    "            'Metric': ['Total Records', 'Unique Locations', 'Successful Queries',\n",
    "                      'Mean Roads', 'Mean Industrial', 'Mean Dumps', 'Mean Agriculture',\n",
    "                      'Max Roads', 'Max Industrial', 'Max Dumps', 'Max Agriculture'],\n",
    "            'Value': [len(merged_df), len(features_df), successful,\n",
    "                     f\"{features_df['Roads_count'].mean():.1f}\",\n",
    "                     f\"{features_df['Industrial_zones_count'].mean():.1f}\",\n",
    "                     f\"{features_df['Dump_sites_count'].mean():.1f}\",\n",
    "                     f\"{features_df['Agricultural_fields_count'].mean():.1f}\",\n",
    "                     features_df['Roads_count'].max(),\n",
    "                     features_df['Industrial_zones_count'].max(),\n",
    "                     features_df['Dump_sites_count'].max(),\n",
    "                     features_df['Agricultural_fields_count'].max()]\n",
    "        })\n",
    "        summary_stats.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    print(f\"âœ“ Successfully saved to {OUTPUT_FILE}\\n\")\n",
    "    \n",
    "    # Display sample results\n",
    "    print(f\"{'Sample Results (First 15 locations):':^80}\")\n",
    "    print(merged_df[['district', 'state', 'Roads_count',\n",
    "                     'Industrial_zones_count', 'Dump_sites_count', 'Agricultural_fields_count',\n",
    "                     'Pollution_source_risk', 'Green_area_ratio']].head(15))\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"âœ“ Processing Complete!\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return merged_df, features_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merged_df, features_df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e859949-0cbd-45e7-ab17-b4c235337c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"Air_Quality_with_OSM_Features_5KM.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96907d-aa1a-497b-b4cf-058bdbf92c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec63c18-ef52-4216-8d62-accdec44fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel(\"Air_Quality_with_OSM_Features_5KM.xlsx\")\n",
    "\n",
    "# 1. Remove records with invalid coordinates\n",
    "df = df[(df[\"latitude\"].between(-90, 90)) & (df[\"longitude\"].between(-180, 180))]\n",
    "\n",
    "# 2. Remove records with missing timestamps\n",
    "df = df.dropna(subset=[\"datetime_utc\"])\n",
    "\n",
    "# 3. Remove records where ALL pollutant values are missing\n",
    "pollutant_cols = [\"pm25\", \"pm10\", \"no2\", \"so2\", \"o3\", \"co\"]  # adjust to your dataset\n",
    "df = df.dropna(how=\"all\", subset=pollutant_cols)\n",
    "\n",
    "# 4. Remove outliers (values outside 3 standard deviations)\n",
    "for col in pollutant_cols:\n",
    "    if col in df.columns:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        df = df[(df[col] >= mean - 3*std) & (df[col] <= mean + 3*std) | df[col].isna()]\n",
    "\n",
    "# 5. Only filter if values are unrealistic (negative or extremely high)\n",
    "for col in pollutant_cols:\n",
    "    if col in df.columns:\n",
    "        df = df[(df[col] >= 0) & (df[col] <= 500) | df[col].isna()]  # AQI realistic range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91dbba0-9dc9-4a02-8d6f-62bfd2047bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a91ae-53a2-49cf-a4dc-8f03d9744cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pollutant_cols = [\"pm25\", \"pm10\", \"no2\", \"co\", \"so2\", \"o3\"]\n",
    "weather_cols=['temperature','humidity','wind_speed','wind_direction']\n",
    "\n",
    "for col in pollutant_cols:\n",
    "    if col in df.columns:\n",
    "        median_val = df[col].median()\n",
    "        df[col]=df[col].fillna(median_val)\n",
    "for col in weather_cols:\n",
    "    if col in df.columns:\n",
    "        median_val = df[col].median()\n",
    "        df[col]=df[col].fillna(median_val)        \n",
    "\n",
    "print(df[pollutant_cols].isnull().sum())\n",
    "print(df[weather_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc31ec9-b780-47fe-b158-553a8d96293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standarsdizing timestamps\")\n",
    "\n",
    "# Convert datetime_utc to datetime object\n",
    "print(\"\\nConverting datetime strings to datetime objects...\")\n",
    "df['datetime_utc'] = pd.to_datetime(df['datetime_utc'], utc=True)\n",
    "\n",
    "# Convert to IST (Indian Standard Time) for local analysis\n",
    "df['datetime_ist'] = df['datetime_utc'].dt.tz_convert('Asia/Kolkata')\n",
    "\n",
    "# Extract date components\n",
    "print(\"Extracting date components...\")\n",
    "df['date'] = df['datetime_ist'].dt.date\n",
    "df['year'] = df['datetime_ist'].dt.year\n",
    "df['month'] = df['datetime_ist'].dt.month\n",
    "df['day'] = df['datetime_ist'].dt.day\n",
    "df['hour'] = df['datetime_ist'].dt.hour\n",
    "df['day_of_week'] = df['datetime_ist'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df['day_name'] = df['datetime_ist'].dt.day_name()\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Round coordinates to standard precision\n",
    "print(\"Standardizing GPS coordinates...\")\n",
    "\n",
    "df['latitude'] = df['latitude'].round(6)\n",
    "df['longitude'] = df['longitude'].round(6)\n",
    "\n",
    "print(\"Sample of standardized data:\")\n",
    "print(\"-\"*50)\n",
    "print(df[['datetime_ist', 'date', 'hour', 'day_of_week', 'is_weekend']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792126be-b073-4095-841a-a1ddf6ec9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a771c-f461-4a9f-babb-7e46af13124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "df = pd.read_excel(\"Air_Quality_with_OSM_Features_5KM.xlsx\")\n",
    "\n",
    "# Define columns\n",
    "pollutant_cols = ['pm25', 'pm10', 'no2', 'co', 'so2', 'o3']\n",
    "weather_cols = ['temperature', 'humidity', 'wind_speed', 'wind_direction']\n",
    "cols_to_normalize = [col for col in pollutant_cols + weather_cols if col in df.columns]\n",
    "\n",
    "print(\"Columns to normalize:\", cols_to_normalize)\n",
    "\n",
    "# --- Min-Max Normalization (0-1 range) ---\n",
    "print(\"\\nApplying Min-Max normalization (0-1 range)...\")\n",
    "minmax_scaler = MinMaxScaler()\n",
    "for col in cols_to_normalize:\n",
    "    df[col + \"_normalized\"] = minmax_scaler.fit_transform(df[[col]])\n",
    "    print(f\"  âœ“ {col}_normalized created\")\n",
    "\n",
    "# --- Standard Scaling (Z-score) ---\n",
    "print(\"\\nApplying Standard scaling (z-score)...\")\n",
    "standard_scaler = StandardScaler()\n",
    "for col in cols_to_normalize:\n",
    "    df[col + \"_scaled\"] = standard_scaler.fit_transform(df[[col]])\n",
    "    print(f\"  âœ“ {col}_scaled created\")\n",
    "\n",
    "print(\"\\nâœ… Normalization complete!\")\n",
    "\n",
    "# --- Sample Output for one feature (pm25) ---\n",
    "print(\"\\n--------------------------------------------------\")\n",
    "print(\"Sample Normalized Values:\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(df[['pm25', 'pm25_normalized', 'pm25_scaled']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4a7d1-105c-47d0-ac29-8048fcc70fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a58eb-e0f4-4448-941c-f13fa81ac5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3cacd7-ee22-452e-aef4-21a59b006d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrosm import OSM\n",
    "print(\"pyrosm imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74bde64-6378-4887-98ae-d4644080137c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.exists(r\"data\\roads.geojson\"), os.path.exists(r\"data\\industrial.geojson\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3222be8-6001-4444-a827-134f9ca185f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading input Excel...\n",
      "ðŸ—ºï¸ Loading OSM features from small PBFs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Riya\\Batch-6\\miniconda3\\envs\\geo\\lib\\site-packages\\pyogrio\\geopandas.py:265: UserWarning: More than one layer found in 'roads.osm.pbf': 'points' (default), 'lines', 'multilinestrings', 'multipolygons', 'other_relations'. Specify layer parameter to avoid this warning.\n",
      "  result = read_func(\n",
      "C:\\Riya\\Batch-6\\miniconda3\\envs\\geo\\lib\\site-packages\\pyogrio\\geopandas.py:265: UserWarning: More than one layer found in 'industrial.osm.pbf': 'points' (default), 'lines', 'multilinestrings', 'multipolygons', 'other_relations'. Specify layer parameter to avoid this warning.\n",
      "  result = read_func(\n",
      "C:\\Riya\\Batch-6\\miniconda3\\envs\\geo\\lib\\site-packages\\pyogrio\\geopandas.py:265: UserWarning: More than one layer found in 'dump_sites.osm.pbf': 'points' (default), 'lines', 'multilinestrings', 'multipolygons', 'other_relations'. Specify layer parameter to avoid this warning.\n",
      "  result = read_func(\n",
      "C:\\Riya\\Batch-6\\miniconda3\\envs\\geo\\lib\\site-packages\\pyogrio\\geopandas.py:265: UserWarning: More than one layer found in 'agriculture.osm.pbf': 'points' (default), 'lines', 'multilinestrings', 'multipolygons', 'other_relations'. Specify layer parameter to avoid this warning.\n",
      "  result = read_func(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Computing distances in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:02<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving final dataset...\n",
      "âœ… DONE â€” Spatial proximity features computed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import box\n",
    "from shapely.strtree import STRtree\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------\n",
    "# PATHS\n",
    "# ---------------------------\n",
    "INPUT_EXCEL = \"Air_Quality_with_OSM_Features_5KM.xlsx\"\n",
    "OUTPUT_EXCEL = \"dataset_with_spatial_proximity_features.xlsx\"\n",
    "\n",
    "roads_fp = r\"data/roads.osm.pbf\"\n",
    "industrial_fp = r\"data/industrial.osm.pbf\"\n",
    "dump_fp = r\"data/dump_sites.osm.pbf\"\n",
    "agri_fp = r\"data/agriculture.osm.pbf\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1. LOAD INPUT DATA\n",
    "# ---------------------------\n",
    "print(\"ðŸ“¥ Loading input Excel...\")\n",
    "df = pd.read_excel(INPUT_EXCEL)\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(epsg=3857)\n",
    "\n",
    "points = gdf.geometry.values\n",
    "\n",
    "# ---------------------------\n",
    "# 2. LOAD OSM FEATURES (SMALL PBFs)\n",
    "# ---------------------------\n",
    "print(\"ðŸ—ºï¸ Loading OSM features from small PBFs...\")\n",
    "\n",
    "roads = gpd.read_file(roads_fp).to_crs(3857)\n",
    "industrial = gpd.read_file(industrial_fp).to_crs(3857)\n",
    "dump_sites = gpd.read_file(dump_fp).to_crs(3857)\n",
    "agriculture = gpd.read_file(agri_fp).to_crs(3857)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. FILTER FEATURES TO POINTS BBOX\n",
    "# ---------------------------\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "bbox_poly = box(minx, miny, maxx, maxy)\n",
    "\n",
    "roads = roads[roads.intersects(bbox_poly)]\n",
    "industrial = industrial[industrial.intersects(bbox_poly)]\n",
    "dump_sites = dump_sites[dump_sites.intersects(bbox_poly)]\n",
    "agriculture = agriculture[agriculture.intersects(bbox_poly)]\n",
    "\n",
    "# ---------------------------\n",
    "# 4. BUILD STRtree INDEXES\n",
    "# ---------------------------\n",
    "road_tree = STRtree(roads.geometry.values)\n",
    "ind_tree = STRtree(industrial.geometry.values)\n",
    "dump_tree = STRtree(dump_sites.geometry.values)\n",
    "agri_tree = STRtree(agriculture.geometry.values)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. VECTORIZE DISTANCE FUNCTION\n",
    "# ---------------------------\n",
    "def min_distance_vectorized(tree, points):\n",
    "    result = np.zeros(len(points))\n",
    "    for i, p in enumerate(points):\n",
    "        candidates = tree.query(p)\n",
    "        if len(candidates) == 0:\n",
    "            result[i] = np.nan\n",
    "        else:\n",
    "            dists = np.array([p.distance(g) for g in candidates])\n",
    "            result[i] = dists.min()\n",
    "    return result\n",
    "\n",
    "# ---------------------------\n",
    "# 6. BATCH PROCESSING (SAFE FOR MEMORY)\n",
    "# ---------------------------\n",
    "batch_size = 5000\n",
    "n = len(points)\n",
    "\n",
    "dist_road = np.zeros(n)\n",
    "dist_ind = np.zeros(n)\n",
    "dist_dump = np.zeros(n)\n",
    "dist_agri = np.zeros(n)\n",
    "\n",
    "print(\"â³ Computing distances in batches...\")\n",
    "for start in tqdm(range(0, n, batch_size), desc=\"Batches\"):\n",
    "    end = min(start + batch_size, n)\n",
    "    batch_points = points[start:end]\n",
    "\n",
    "    dist_road[start:end] = min_distance_vectorized(road_tree, batch_points)\n",
    "    dist_ind[start:end] = min_distance_vectorized(ind_tree, batch_points)\n",
    "    dist_dump[start:end] = min_distance_vectorized(dump_tree, batch_points)\n",
    "    dist_agri[start:end] = min_distance_vectorized(agri_tree, batch_points)\n",
    "\n",
    "# ---------------------------\n",
    "# 7. SAVE OUTPUT\n",
    "# ---------------------------\n",
    "gdf[\"dist_nearest_road_m\"] = dist_road\n",
    "gdf[\"dist_nearest_industry_m\"] = dist_ind\n",
    "gdf[\"dist_nearest_dump_m\"] = dist_dump\n",
    "gdf[\"dist_nearest_agriculture_m\"] = dist_agri\n",
    "\n",
    "print(\"ðŸ’¾ Saving final dataset...\")\n",
    "gdf.drop(columns=\"geometry\").to_excel(OUTPUT_EXCEL, index=False)\n",
    "\n",
    "print(\"âœ… DONE â€” Spatial proximity features computed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c58df96-c049-4e36-913a-70319f7a6235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Temporal features added and saved to Excel!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SINGLE-CELL: TEMPORAL FEATURE ENGINEERING (EXCEL)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load Excel file\n",
    "# -----------------------------\n",
    "df = pd.read_excel(\"dataset_with_spatial_proximity_features.xlsx\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Parse datetime column\n",
    "# -----------------------------\n",
    "df[\"datetime_local\"] = pd.to_datetime(df[\"datetime_local\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Hour-based features\n",
    "# -----------------------------\n",
    "df[\"hour\"] = df[\"datetime_local\"].dt.hour\n",
    "df[\"is_peak_hour\"] = df[\"hour\"].isin([7, 8, 9, 18, 19, 20]).astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Day-based features\n",
    "# -----------------------------\n",
    "df[\"day_of_week\"] = df[\"datetime_local\"].dt.dayofweek   # 0=Mon, 6=Sun\n",
    "df[\"is_weekend\"] = (df[\"day_of_week\"] >= 5).astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Month & season features (India)\n",
    "# -----------------------------\n",
    "df[\"month\"] = df[\"datetime_local\"].dt.month\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Summer\"\n",
    "    elif month in [6, 7, 8, 9]:\n",
    "        return \"Monsoon\"\n",
    "    else:\n",
    "        return \"Post_Monsoon\"\n",
    "\n",
    "df[\"season\"] = df[\"month\"].apply(get_season)\n",
    "\n",
    "# Numeric encoding (ML-ready)\n",
    "season_map = {\n",
    "    \"Winter\": 0,\n",
    "    \"Summer\": 1,\n",
    "    \"Monsoon\": 2,\n",
    "    \"Post_Monsoon\": 3\n",
    "}\n",
    "df[\"season_code\"] = df[\"season\"].map(season_map)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Remove timezone info\n",
    "# -----------------------------\n",
    "df[\"datetime_local\"] = df[\"datetime_local\"].dt.tz_localize(None)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Save back to Excel\n",
    "# -----------------------------\n",
    "df.to_excel(\n",
    "    \"dataset_with_spatial_and_temporal_features.xlsx\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Temporal features added and saved to Excel!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a7430b-58d4-4872-a294-b51c20f4cb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loaded dataset (77994 rows, 38 columns)\n",
      "Columns to normalize: ['pm25', 'pm10', 'no2', 'co', 'so2', 'o3', 'temperature', 'humidity', 'wind_speed', 'wind_direction']\n",
      "âœ… Missing values filled with median\n",
      "âœ… Min-Max normalization done\n",
      "âœ… Standard scaling done\n",
      "\n",
      "ðŸ’¾ Dataset saved: Final_dataset.xlsx\n",
      "\n",
      "Sample (PM2.5):\n",
      "                pm25  pm25_normalized   pm25_scaled\n",
      "count   77994.000000     77994.000000  7.799400e+04\n",
      "mean      180.638641         0.011834 -1.271664e-09\n",
      "std      6087.090820         0.007076  1.000006e+00\n",
      "min     -9999.000000         0.000000 -1.672343e+00\n",
      "25%        29.570000         0.011659 -2.481803e-02\n",
      "50%        46.279999         0.011678 -2.207286e-02\n",
      "75%        73.757502         0.011710 -1.755877e-02\n",
      "max    850191.250000         1.000000  1.396424e+02\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MEMORY-EFFICIENT NORMALIZATION OF POLLUTANT & WEATHER FEATURES\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load dataset\n",
    "# -----------------------------\n",
    "INPUT_FILE = \"dataset_with_spatial_and_temporal_features.xlsx\"\n",
    "OUTPUT_FILE = \"Final_dataset.xlsx\"\n",
    "\n",
    "df = pd.read_excel(INPUT_FILE)\n",
    "print(f\"ðŸ“¥ Loaded dataset ({df.shape[0]} rows, {df.shape[1]} columns)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Define columns\n",
    "# -----------------------------\n",
    "pollutant_cols = ['pm25', 'pm10', 'no2', 'co', 'so2', 'o3']\n",
    "weather_cols = ['temperature', 'humidity', 'wind_speed', 'wind_direction']\n",
    "\n",
    "cols_to_normalize = [col for col in pollutant_cols + weather_cols if col in df.columns]\n",
    "print(\"Columns to normalize:\", cols_to_normalize)\n",
    "\n",
    "# Convert to float32 to save memory\n",
    "df[cols_to_normalize] = df[cols_to_normalize].astype(np.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Handle missing values (median)\n",
    "# -----------------------------\n",
    "df[cols_to_normalize] = df[cols_to_normalize].fillna(df[cols_to_normalize].median())\n",
    "print(\"âœ… Missing values filled with median\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Min-Max Normalization (0â€“1)\n",
    "# -----------------------------\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df[[f\"{col}_normalized\" for col in cols_to_normalize]] = minmax_scaler.fit_transform(df[cols_to_normalize])\n",
    "print(\"âœ… Min-Max normalization done\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Standard Scaling (Z-score)\n",
    "# -----------------------------\n",
    "standard_scaler = StandardScaler()\n",
    "df[[f\"{col}_scaled\" for col in cols_to_normalize]] = standard_scaler.fit_transform(df[cols_to_normalize])\n",
    "print(\"âœ… Standard scaling done\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Save final dataset\n",
    "# -----------------------------\n",
    "df.to_excel(OUTPUT_FILE, index=False)\n",
    "print(f\"\\nðŸ’¾ Dataset saved: {OUTPUT_FILE}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Quick check\n",
    "# -----------------------------\n",
    "print(\"\\nSample (PM2.5):\")\n",
    "print(df[['pm25', 'pm25_normalized', 'pm25_scaled']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73495900-7896-4e80-b263-d4a4c9cee92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully merged dataset with spatial, temporal, and normalized features!\n",
      "Shape: (77994, 94)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SAFE MERGE: SPATIAL-TEMPORAL + NORMALIZED DATA\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load datasets\n",
    "# -----------------------------\n",
    "spatial_temporal = pd.read_excel(\"dataset_with_spatial_and_temporal_features.xlsx\")\n",
    "normalized = pd.read_excel(\"Final_dataset.xlsx\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Remove timezone from datetime (if any)\n",
    "# -----------------------------\n",
    "if isinstance(spatial_temporal[\"datetime_local\"].dtype, pd.DatetimeTZDtype):\n",
    "    spatial_temporal[\"datetime_local\"] = spatial_temporal[\"datetime_local\"].dt.tz_localize(None)\n",
    "\n",
    "if isinstance(normalized[\"datetime_local\"].dtype, pd.DatetimeTZDtype):\n",
    "    normalized[\"datetime_local\"] = normalized[\"datetime_local\"].dt.tz_localize(None)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Define merge columns\n",
    "# -----------------------------\n",
    "merge_cols = [\"location_id\", \"datetime_local\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Merge datasets\n",
    "# -----------------------------\n",
    "final_df = pd.merge(\n",
    "    spatial_temporal,\n",
    "    normalized,\n",
    "    on=merge_cols,\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_norm\")  # avoids column name clashes\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Save final dataset\n",
    "# -----------------------------\n",
    "final_df.to_excel(\"Final_merged_dataset.xlsx\", index=False)\n",
    "\n",
    "print(\"âœ… Successfully merged dataset with spatial, temporal, and normalized features!\")\n",
    "print(f\"Shape: {final_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c1496b-83e5-4244-87b3-aca4985e989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_temporal[\"datetime_local\"] = pd.to_datetime(spatial_temporal[\"datetime_local\"])\n",
    "normalized[\"datetime_local\"] = pd.to_datetime(normalized[\"datetime_local\"])\n",
    "\n",
    "spatial_temporal[\"location_id\"] = spatial_temporal[\"location_id\"].astype(\"int32\")\n",
    "normalized[\"location_id\"] = normalized[\"location_id\"].astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dce1d3d-0586-4b65-8467-46e70866a04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['state_norm', 'district_norm', 'location_name_norm',\n",
       "       'datetime_utc_norm', 'latitude_norm', 'longitude_norm', 'pm25_norm',\n",
       "       'pm10_norm', 'no2_norm', 'co_norm', 'so2_norm', 'o3_norm',\n",
       "       'temperature_norm', 'humidity_norm', 'wind_speed_norm',\n",
       "       'wind_direction_norm', 'Roads_count_norm',\n",
       "       'Industrial_zones_count_norm', 'Dump_sites_count_norm',\n",
       "       'Agricultural_fields_count_norm', 'Query_status_norm',\n",
       "       'Urban_density_score_norm', 'Industrial_presence_norm',\n",
       "       'Pollution_source_risk_norm', 'Green_area_ratio_norm',\n",
       "       'dist_nearest_road_m_norm', 'dist_nearest_industry_m_norm',\n",
       "       'dist_nearest_dump_m_norm', 'dist_nearest_agriculture_m_norm',\n",
       "       'hour_norm', 'is_peak_hour_norm', 'day_of_week_norm', 'is_weekend_norm',\n",
       "       'month_norm', 'season_norm', 'season_code_norm', 'pm25_normalized',\n",
       "       'pm10_normalized', 'no2_normalized', 'co_normalized', 'so2_normalized',\n",
       "       'o3_normalized', 'temperature_normalized', 'humidity_normalized',\n",
       "       'wind_speed_normalized', 'wind_direction_normalized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns[final_df.columns.str.contains(\"norm\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba6e712-0881-4878-a14a-cf2c1d38c2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 36 redundant columns\n",
      "New shape: (77994, 58)\n"
     ]
    }
   ],
   "source": [
    "# Drop all *_norm columns\n",
    "cols_to_drop = [col for col in final_df.columns if col.endswith(\"_norm\")]\n",
    "final_df = final_df.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Removed {len(cols_to_drop)} redundant columns\")\n",
    "print(f\"New shape: {final_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5aa82b6-f621-4c70-8517-18dac6e5a826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>district</th>\n",
       "      <th>location_id</th>\n",
       "      <th>location_name</th>\n",
       "      <th>datetime_utc</th>\n",
       "      <th>datetime_local</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>pm25</th>\n",
       "      <th>pm10</th>\n",
       "      <th>...</th>\n",
       "      <th>pm25_scaled</th>\n",
       "      <th>pm10_scaled</th>\n",
       "      <th>no2_scaled</th>\n",
       "      <th>co_scaled</th>\n",
       "      <th>so2_scaled</th>\n",
       "      <th>o3_scaled</th>\n",
       "      <th>temperature_scaled</th>\n",
       "      <th>humidity_scaled</th>\n",
       "      <th>wind_speed_scaled</th>\n",
       "      <th>wind_direction_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>Tirupati</td>\n",
       "      <td>5649</td>\n",
       "      <td>Tirumala, Tirupati - APPCB</td>\n",
       "      <td>2025-11-11T15:00:00Z</td>\n",
       "      <td>2025-11-11 20:30:00</td>\n",
       "      <td>13.67</td>\n",
       "      <td>79.35</td>\n",
       "      <td>81.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016369</td>\n",
       "      <td>-0.048343</td>\n",
       "      <td>-0.027710</td>\n",
       "      <td>0.093771</td>\n",
       "      <td>-0.372981</td>\n",
       "      <td>-0.005949</td>\n",
       "      <td>0.202941</td>\n",
       "      <td>1.059692</td>\n",
       "      <td>-0.188803</td>\n",
       "      <td>2.060052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>Tirupati</td>\n",
       "      <td>5649</td>\n",
       "      <td>Tirumala, Tirupati - APPCB</td>\n",
       "      <td>2025-11-11T15:15:00Z</td>\n",
       "      <td>2025-11-11 20:45:00</td>\n",
       "      <td>13.67</td>\n",
       "      <td>79.35</td>\n",
       "      <td>81.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016369</td>\n",
       "      <td>-0.048343</td>\n",
       "      <td>-0.035966</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>-0.372981</td>\n",
       "      <td>-0.005949</td>\n",
       "      <td>0.181327</td>\n",
       "      <td>1.059692</td>\n",
       "      <td>-0.194356</td>\n",
       "      <td>2.060052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>Tirupati</td>\n",
       "      <td>5649</td>\n",
       "      <td>Tirumala, Tirupati - APPCB</td>\n",
       "      <td>2025-11-11T15:45:00Z</td>\n",
       "      <td>2025-11-11 21:15:00</td>\n",
       "      <td>13.67</td>\n",
       "      <td>79.35</td>\n",
       "      <td>81.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016369</td>\n",
       "      <td>-0.048343</td>\n",
       "      <td>-0.030092</td>\n",
       "      <td>0.186179</td>\n",
       "      <td>-0.369372</td>\n",
       "      <td>-0.005949</td>\n",
       "      <td>0.170521</td>\n",
       "      <td>1.059692</td>\n",
       "      <td>-0.199909</td>\n",
       "      <td>2.060052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>Tirupati</td>\n",
       "      <td>5649</td>\n",
       "      <td>Tirumala, Tirupati - APPCB</td>\n",
       "      <td>2025-11-11T16:15:00Z</td>\n",
       "      <td>2025-11-11 21:45:00</td>\n",
       "      <td>13.67</td>\n",
       "      <td>79.35</td>\n",
       "      <td>90.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014890</td>\n",
       "      <td>-0.049869</td>\n",
       "      <td>-0.032791</td>\n",
       "      <td>0.067369</td>\n",
       "      <td>-0.372981</td>\n",
       "      <td>-0.005949</td>\n",
       "      <td>0.202941</td>\n",
       "      <td>0.939240</td>\n",
       "      <td>-0.199909</td>\n",
       "      <td>2.060052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>Tirupati</td>\n",
       "      <td>5649</td>\n",
       "      <td>Tirumala, Tirupati - APPCB</td>\n",
       "      <td>2025-11-11T16:30:00Z</td>\n",
       "      <td>2025-11-11 22:00:00</td>\n",
       "      <td>13.67</td>\n",
       "      <td>79.35</td>\n",
       "      <td>90.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014890</td>\n",
       "      <td>-0.049869</td>\n",
       "      <td>-0.043429</td>\n",
       "      <td>-0.091043</td>\n",
       "      <td>-0.376589</td>\n",
       "      <td>-0.005949</td>\n",
       "      <td>0.192134</td>\n",
       "      <td>0.979391</td>\n",
       "      <td>-0.194356</td>\n",
       "      <td>2.060052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            state  district  location_id               location_name  \\\n",
       "0  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
       "1  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
       "2  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
       "3  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
       "4  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
       "\n",
       "           datetime_utc      datetime_local  latitude  longitude  pm25   pm10  \\\n",
       "0  2025-11-11T15:00:00Z 2025-11-11 20:30:00     13.67      79.35  81.0  115.0   \n",
       "1  2025-11-11T15:15:00Z 2025-11-11 20:45:00     13.67      79.35  81.0  115.0   \n",
       "2  2025-11-11T15:45:00Z 2025-11-11 21:15:00     13.67      79.35  81.0  115.0   \n",
       "3  2025-11-11T16:15:00Z 2025-11-11 21:45:00     13.67      79.35  90.0  114.0   \n",
       "4  2025-11-11T16:30:00Z 2025-11-11 22:00:00     13.67      79.35  90.0  114.0   \n",
       "\n",
       "   ...  pm25_scaled  pm10_scaled  no2_scaled  co_scaled  so2_scaled  \\\n",
       "0  ...    -0.016369    -0.048343   -0.027710   0.093771   -0.372981   \n",
       "1  ...    -0.016369    -0.048343   -0.035966   0.001364   -0.372981   \n",
       "2  ...    -0.016369    -0.048343   -0.030092   0.186179   -0.369372   \n",
       "3  ...    -0.014890    -0.049869   -0.032791   0.067369   -0.372981   \n",
       "4  ...    -0.014890    -0.049869   -0.043429  -0.091043   -0.376589   \n",
       "\n",
       "   o3_scaled  temperature_scaled  humidity_scaled  wind_speed_scaled  \\\n",
       "0  -0.005949            0.202941         1.059692          -0.188803   \n",
       "1  -0.005949            0.181327         1.059692          -0.194356   \n",
       "2  -0.005949            0.170521         1.059692          -0.199909   \n",
       "3  -0.005949            0.202941         0.939240          -0.199909   \n",
       "4  -0.005949            0.192134         0.979391          -0.194356   \n",
       "\n",
       "   wind_direction_scaled  \n",
       "0               2.060052  \n",
       "1               2.060052  \n",
       "2               2.060052  \n",
       "3               2.060052  \n",
       "4               2.060052  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_df = pd.read_excel(\"Final_merged_dataset_CLEAN.xlsx\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f3aebe-873d-4c9f-b3a1-cc4d807aa382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state                             0\n",
       "district                          0\n",
       "location_id                       0\n",
       "location_name                     0\n",
       "datetime_utc                      0\n",
       "datetime_local                    0\n",
       "latitude                          0\n",
       "longitude                         0\n",
       "pm25                           5360\n",
       "pm10                           8972\n",
       "no2                            6092\n",
       "co                             9165\n",
       "so2                            6219\n",
       "o3                            12926\n",
       "temperature                   13372\n",
       "humidity                      14519\n",
       "wind_speed                    14406\n",
       "wind_direction                14459\n",
       "Roads_count                       0\n",
       "Industrial_zones_count            0\n",
       "Dump_sites_count                  0\n",
       "Agricultural_fields_count         0\n",
       "Query_status                      0\n",
       "Urban_density_score               0\n",
       "Industrial_presence               0\n",
       "Pollution_source_risk             0\n",
       "Green_area_ratio                  0\n",
       "dist_nearest_road_m           77994\n",
       "dist_nearest_industry_m       77994\n",
       "dist_nearest_dump_m           77994\n",
       "dist_nearest_agriculture_m    77994\n",
       "hour                              0\n",
       "is_peak_hour                      0\n",
       "day_of_week                       0\n",
       "is_weekend                        0\n",
       "month                             0\n",
       "season                            0\n",
       "season_code                       0\n",
       "pm25_normalized                   0\n",
       "pm10_normalized                   0\n",
       "no2_normalized                    0\n",
       "co_normalized                     0\n",
       "so2_normalized                    0\n",
       "o3_normalized                     0\n",
       "temperature_normalized            0\n",
       "humidity_normalized               0\n",
       "wind_speed_normalized             0\n",
       "wind_direction_normalized         0\n",
       "pm25_scaled                       0\n",
       "pm10_scaled                       0\n",
       "no2_scaled                        0\n",
       "co_scaled                         0\n",
       "so2_scaled                        0\n",
       "o3_scaled                         0\n",
       "temperature_scaled                0\n",
       "humidity_scaled                   0\n",
       "wind_speed_scaled                 0\n",
       "wind_direction_scaled             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22caf023-e3c9-4559-aa02-1de39a5ea08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading FINAL dataset...\n",
      "ðŸ—ºï¸ Loading OSM layers...\n",
      "Features after clip: 9427153 21573 773 84265\n",
      "â³ Recomputing spatial proximity distances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:10<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving corrected FINAL dataset...\n",
      "âœ… DONE â€” Spatial proximity columns FIXED in final dataset!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "from shapely.strtree import STRtree\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------\n",
    "# FILE PATHS\n",
    "# ---------------------------\n",
    "INPUT_EXCEL = \"Final_merged_dataset_CLEAN.xlsx\"\n",
    "OUTPUT_EXCEL = \"Finalised_merged_dataset.xlsx\"\n",
    "\n",
    "roads_fp = r\"data/roads.osm.pbf\"\n",
    "industrial_fp = r\"data/industrial.osm.pbf\"\n",
    "dump_fp = r\"data/dump_sites.osm.pbf\"\n",
    "agri_fp = r\"data/agriculture.osm.pbf\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1. LOAD FINAL DATASET\n",
    "# ---------------------------\n",
    "print(\"ðŸ“¥ Loading FINAL dataset...\")\n",
    "df = pd.read_excel(INPUT_EXCEL)\n",
    "\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(epsg=3857)  # meters\n",
    "\n",
    "points = gdf.geometry.values\n",
    "n = len(points)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. LOAD OSM FEATURES\n",
    "# ---------------------------\n",
    "print(\"ðŸ—ºï¸ Loading OSM layers...\")\n",
    "\n",
    "roads = gpd.read_file(roads_fp, layer=\"lines\").to_crs(3857)\n",
    "\n",
    "industrial = gpd.read_file(\n",
    "    industrial_fp, layer=\"multipolygons\"\n",
    ").to_crs(3857)\n",
    "\n",
    "dump_sites = gpd.read_file(\n",
    "    dump_fp, layer=\"multipolygons\"\n",
    ").to_crs(3857)\n",
    "\n",
    "agriculture = gpd.read_file(\n",
    "    agri_fp, layer=\"multipolygons\"\n",
    ").to_crs(3857)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. CLIP TO POINT BOUNDING BOX\n",
    "# ---------------------------\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "bbox = box(minx, miny, maxx, maxy)\n",
    "\n",
    "roads = roads[roads.intersects(bbox)]\n",
    "industrial = industrial[industrial.intersects(bbox)]\n",
    "dump_sites = dump_sites[dump_sites.intersects(bbox)]\n",
    "agriculture = agriculture[agriculture.intersects(bbox)]\n",
    "\n",
    "print(\"Features after clip:\",\n",
    "      len(roads), len(industrial), len(dump_sites), len(agriculture))\n",
    "\n",
    "# ---------------------------\n",
    "# 4. BUILD STRtree (CORRECT WAY)\n",
    "# ---------------------------\n",
    "road_geoms = roads.geometry.values\n",
    "ind_geoms = industrial.geometry.values\n",
    "dump_geoms = dump_sites.geometry.values\n",
    "agri_geoms = agriculture.geometry.values\n",
    "\n",
    "road_tree = STRtree(road_geoms)\n",
    "ind_tree = STRtree(ind_geoms)\n",
    "dump_tree = STRtree(dump_geoms)\n",
    "agri_tree = STRtree(agri_geoms)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. FIXED DISTANCE FUNCTION\n",
    "# ---------------------------\n",
    "def compute_min_distance(tree, geometries, pts):\n",
    "    out = np.empty(len(pts))\n",
    "    for i, p in enumerate(pts):\n",
    "        idx = tree.query(p)  # returns INDICES\n",
    "        if len(idx) == 0:\n",
    "            out[i] = np.nan\n",
    "        else:\n",
    "            out[i] = min(p.distance(geometries[j]) for j in idx)\n",
    "    return out\n",
    "\n",
    "# ---------------------------\n",
    "# 6. BATCH COMPUTATION\n",
    "# ---------------------------\n",
    "batch_size = 5000\n",
    "\n",
    "dist_road = np.empty(n)\n",
    "dist_ind = np.empty(n)\n",
    "dist_dump = np.empty(n)\n",
    "dist_agri = np.empty(n)\n",
    "\n",
    "print(\"â³ Recomputing spatial proximity distances...\")\n",
    "for start in tqdm(range(0, n, batch_size)):\n",
    "    end = min(start + batch_size, n)\n",
    "    batch_pts = points[start:end]\n",
    "\n",
    "    dist_road[start:end] = compute_min_distance(road_tree, road_geoms, batch_pts)\n",
    "    dist_ind[start:end] = compute_min_distance(ind_tree, ind_geoms, batch_pts)\n",
    "    dist_dump[start:end] = compute_min_distance(dump_tree, dump_geoms, batch_pts)\n",
    "    dist_agri[start:end] = compute_min_distance(agri_tree, agri_geoms, batch_pts)\n",
    "\n",
    "# ---------------------------\n",
    "# 7. UPDATE ONLY SPATIAL COLUMNS\n",
    "# ---------------------------\n",
    "gdf[\"dist_nearest_road_m\"] = dist_road\n",
    "gdf[\"dist_nearest_industry_m\"] = dist_ind\n",
    "gdf[\"dist_nearest_dump_m\"] = dist_dump\n",
    "gdf[\"dist_nearest_agriculture_m\"] = dist_agri\n",
    "\n",
    "# ---------------------------\n",
    "# 8. SAVE FIXED DATASET\n",
    "# ---------------------------\n",
    "print(\"ðŸ’¾ Saving corrected FINAL dataset...\")\n",
    "gdf.drop(columns=\"geometry\").to_excel(OUTPUT_EXCEL, index=False)\n",
    "\n",
    "print(\"âœ… DONE â€” Spatial proximity columns FIXED in final dataset!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8dce09c-3569-4b95-a749-4cdc52085ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LineString MultiPolygon MultiPolygon MultiPolygon\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    roads.geometry.iloc[0].geom_type,\n",
    "    industrial.geometry.iloc[0].geom_type,\n",
    "    dump_sites.geometry.iloc[0].geom_type,\n",
    "    agriculture.geometry.iloc[0].geom_type\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "097fe3b4-ddf9-4f3b-bca0-d15d5df9ad71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist_nearest_road_m</th>\n",
       "      <th>dist_nearest_industry_m</th>\n",
       "      <th>dist_nearest_dump_m</th>\n",
       "      <th>dist_nearest_agriculture_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>70978.000000</td>\n",
       "      <td>13439.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>232.091886</td>\n",
       "      <td>22.968907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>738.065916</td>\n",
       "      <td>37.710140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.595581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26.781288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>65.523674</td>\n",
       "      <td>51.838785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4008.434126</td>\n",
       "      <td>96.170442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dist_nearest_road_m  dist_nearest_industry_m  dist_nearest_dump_m  \\\n",
       "count         70978.000000             13439.000000                  0.0   \n",
       "mean            232.091886                22.968907                  NaN   \n",
       "std             738.065916                37.710140                  NaN   \n",
       "min               0.620448                 0.000000                  NaN   \n",
       "25%              13.595581                 0.000000                  NaN   \n",
       "50%              26.781288                 0.000000                  NaN   \n",
       "75%              65.523674                51.838785                  NaN   \n",
       "max            4008.434126                96.170442                  NaN   \n",
       "\n",
       "       dist_nearest_agriculture_m  \n",
       "count                         0.0  \n",
       "mean                          NaN  \n",
       "std                           NaN  \n",
       "min                           NaN  \n",
       "25%                           NaN  \n",
       "50%                           NaN  \n",
       "75%                           NaN  \n",
       "max                           NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Finalised_merged_dataset.xlsx\")\n",
    "\n",
    "df[\n",
    "    [\n",
    "        \"dist_nearest_road_m\",\n",
    "        \"dist_nearest_industry_m\",\n",
    "        \"dist_nearest_dump_m\",\n",
    "        \"dist_nearest_agriculture_m\"\n",
    "    ]\n",
    "].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "550e4fcb-5932-4fc4-8ca9-7750cb85c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Missing spatial proximity values filled and saved in the existing dataset.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your final dataset\n",
    "df = pd.read_excel(\"Finalised_merged_dataset.xlsx\")\n",
    "\n",
    "# Fill missing distances for dump and agriculture\n",
    "df[\"dist_nearest_dump_m\"] = df[\"dist_nearest_dump_m\"].fillna(10000)\n",
    "df[\"dist_nearest_agriculture_m\"] = df[\"dist_nearest_agriculture_m\"].fillna(10000)\n",
    "\n",
    "# Save changes back to the same file\n",
    "df.to_excel(\"Finalised_merged_dataset.xlsx\", index=False)\n",
    "\n",
    "print(\"âœ… Missing spatial proximity values filled and saved in the existing dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c4f86fc-53d7-4b43-81d9-7818ecf7c014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist_nearest_road_m</th>\n",
       "      <th>dist_nearest_industry_m</th>\n",
       "      <th>dist_nearest_dump_m</th>\n",
       "      <th>dist_nearest_agriculture_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>70978.000000</td>\n",
       "      <td>13439.000000</td>\n",
       "      <td>77994.0</td>\n",
       "      <td>77994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>232.091886</td>\n",
       "      <td>22.968907</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>738.065916</td>\n",
       "      <td>37.710140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.595581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26.781288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>65.523674</td>\n",
       "      <td>51.838785</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4008.434126</td>\n",
       "      <td>96.170442</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dist_nearest_road_m  dist_nearest_industry_m  dist_nearest_dump_m  \\\n",
       "count         70978.000000             13439.000000              77994.0   \n",
       "mean            232.091886                22.968907              10000.0   \n",
       "std             738.065916                37.710140                  0.0   \n",
       "min               0.620448                 0.000000              10000.0   \n",
       "25%              13.595581                 0.000000              10000.0   \n",
       "50%              26.781288                 0.000000              10000.0   \n",
       "75%              65.523674                51.838785              10000.0   \n",
       "max            4008.434126                96.170442              10000.0   \n",
       "\n",
       "       dist_nearest_agriculture_m  \n",
       "count                     77994.0  \n",
       "mean                      10000.0  \n",
       "std                           0.0  \n",
       "min                       10000.0  \n",
       "25%                       10000.0  \n",
       "50%                       10000.0  \n",
       "75%                       10000.0  \n",
       "max                       10000.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\n",
    "    [\n",
    "        \"dist_nearest_road_m\",\n",
    "        \"dist_nearest_industry_m\",\n",
    "        \"dist_nearest_dump_m\",\n",
    "        \"dist_nearest_agriculture_m\"\n",
    "    ]\n",
    "].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6d4cf3d-5db0-4a88-92c7-1c610f84cdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and merging into final dataset completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Data extraction and merging into final dataset completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo)",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
