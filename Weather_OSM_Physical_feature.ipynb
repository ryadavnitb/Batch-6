{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "API_KEY = \"your API key\"   \n",
    "\n",
    "# ==========================\n",
    "# 1. Load CSV File\n",
    "# ==========================\n",
    "file_path = r\"Update with your CSV file path\"  \n",
    "df_cities = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Total cities found:\", len(df_cities))\n",
    "\n",
    "# ==========================\n",
    "# 2. Storage for results\n",
    "# ==========================\n",
    "weather_data = []\n",
    "\n",
    "# ==========================\n",
    "# 3. Fetch weather for each city\n",
    "# ==========================\n",
    "for index, row in df_cities.iterrows():\n",
    "    city = str(row[\"City\"]).strip()    \n",
    "\n",
    "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_KEY}&units=metric\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if \"main\" in data:\n",
    "        weather_data.append({\n",
    "            \"City\": city,\n",
    "            \"Latitude\": data[\"coord\"][\"lat\"],\n",
    "            \"Longitude\": data[\"coord\"][\"lon\"],\n",
    "            \"Temperature (¬∞C)\": data[\"main\"][\"temp\"],\n",
    "            \"Humidity (%)\": data[\"main\"][\"humidity\"],\n",
    "            \"Wind Speed (m/s)\": data[\"wind\"][\"speed\"],\n",
    "            \"Wind Direction (¬∞)\": data[\"wind\"].get(\"deg\", None),\n",
    "            \"Weather Description\": data[\"weather\"][0][\"description\"],\n",
    "            \"Timestamp\": datetime.utcnow()\n",
    "        })\n",
    "\n",
    "        print(f\"‚úî Collected: {city}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ùå Not Found: {city} ‚Üí {data.get('message')}\")\n",
    "\n",
    "    time.sleep(1)  # avoid API rate limit (60 req/min)\n",
    "\n",
    "# ==========================\n",
    "# 4. Save output\n",
    "# ==========================\n",
    "df_weather = pd.DataFrame(weather_data)\n",
    "output_file = \"city_weather_data.csv\"\n",
    "df_weather.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"‚úî Weather data collection DONE!\")\n",
    "print(f\"‚úî Saved File: {output_file}\")\n",
    "print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import osmnx as ox\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# ============================================================\n",
    "#              1. LOAD CITY FILE AND SPLIT INTO 10 CHUNKS\n",
    "# ============================================================\n",
    "\n",
    "INPUT_FILE = \"Update with your CSV file path\"\n",
    "NUM_CHUNKS = 10   # <-- Updated for 10 machines\n",
    "\n",
    "print(\"\\nüìå Loading city file...\")\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "print(f\"Total cities loaded: {len(df)}\")\n",
    "chunks = np.array_split(df, NUM_CHUNKS)\n",
    "\n",
    "chunk_files = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    file = f\"City_chunk_{i+1}.csv\"\n",
    "    chunk.to_csv(file, index=False)\n",
    "    chunk_files.append(file)\n",
    "    print(f\"‚úî Saved chunk {i+1}: {file} ({len(chunk)} cities)\")\n",
    "\n",
    "print(\"\\nüéâ City split into 10 chunks! Starting processing...\\n\")\n",
    "\n",
    "# ============================================================\n",
    "#                2. OSM FEATURE EXTRACTION FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "# OSMnx settings\n",
    "ox.settings.log_console = False\n",
    "ox.settings.use_cache = True\n",
    "ox.settings.cache_folder = \"OSM_CACHE\"\n",
    "ox.settings.overpass_rate_limit = False\n",
    "\n",
    "distance_m = 2000  # 2 km search radius\n",
    "\n",
    "# Feature tags\n",
    "tags = {\n",
    "    'highway': ['primary', 'secondary', 'tertiary', 'residential'], \n",
    "    'landuse': ['industrial', 'farmland', 'landfill', 'forest', 'meadow'],\n",
    "    'amenity': ['waste_disposal', 'recycling'],\n",
    "    'natural': ['wood', 'grassland']\n",
    "}\n",
    "\n",
    "def process_city(city):\n",
    "    print(f\"Processing: {city}\")\n",
    "\n",
    "    # Retry mechanism\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            gdf = ox.features_from_address(city, tags=tags, dist=distance_m)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Retry {attempt+1}/3 for {city} due to {e}\")\n",
    "            time.sleep(1)\n",
    "    else:\n",
    "        print(f\"‚ùå Failed for {city}\")\n",
    "        return {\n",
    "            \"City\": city,\n",
    "            \"Road_Count\": 0,\n",
    "            \"Industrial_Count\": 0,\n",
    "            \"Farmland_Count\": 0,\n",
    "            \"Landfill_Count\": 0,\n",
    "            \"Dump_Site_Count\": 0,\n",
    "            \"Recycling_Count\": 0,\n",
    "            \"Green_Area_Count\": 0\n",
    "        }\n",
    "\n",
    "    def count_feature(col, value=None):\n",
    "        if col not in gdf.columns:\n",
    "            return 0\n",
    "        if value:\n",
    "            return (gdf[col] == value).sum()\n",
    "        return gdf[col].notna().sum()\n",
    "\n",
    "    result = {\n",
    "        \"City\": city,\n",
    "        \"Road_Count\": count_feature(\"highway\"),\n",
    "        \"Industrial_Count\": count_feature(\"landuse\", \"industrial\"),\n",
    "        \"Farmland_Count\": count_feature(\"landuse\", \"farmland\"),\n",
    "        \"Landfill_Count\": count_feature(\"landuse\", \"landfill\"),\n",
    "        \"Dump_Site_Count\": count_feature(\"amenity\", \"waste_disposal\"),\n",
    "        \"Recycling_Count\": count_feature(\"amenity\", \"recycling\"),\n",
    "        \"Green_Area_Count\":\n",
    "            count_feature(\"landuse\", \"forest\") +\n",
    "            count_feature(\"natural\", \"wood\") +\n",
    "            count_feature(\"landuse\", \"meadow\") +\n",
    "            count_feature(\"natural\", \"grassland\")\n",
    "    }\n",
    "\n",
    "    print(f\"‚úî Done: {city}\")\n",
    "    return result\n",
    "\n",
    "# ============================================================\n",
    "#                  3. PROCESS ALL 10 CHUNKS\n",
    "# ============================================================\n",
    "\n",
    "chunk_outputs = []\n",
    "\n",
    "for file in chunk_files:\n",
    "    print(f\"\\nüöÄ Starting processing for: {file}\\n\")\n",
    "    df_chunk = pd.read_csv(file)\n",
    "    cities = df_chunk['City'].dropna().tolist()\n",
    "\n",
    "    results = []\n",
    "    max_threads = 20\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "        futures = {executor.submit(process_city, city): city for city in cities}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            results.append(future.result())\n",
    "\n",
    "    output_file = f\"Processed_{file}\"\n",
    "    pd.DataFrame(results).to_csv(output_file, index=False)\n",
    "    chunk_outputs.append(output_file)\n",
    "    print(f\"\\n‚úî Saved output for {file} ‚Üí {output_file}\")\n",
    "\n",
    "# ============================================================\n",
    "#                   4. MERGE ALL FINAL OUTPUTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìå Merging all processed chunks...\\n\")\n",
    "\n",
    "dfs = [pd.read_csv(f) for f in chunk_outputs]\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "final_output = \"OSM_final_merged_output.csv\"\n",
    "final_df.to_csv(final_output, index=False)\n",
    "\n",
    "print(f\"üéâ ALL DONE! Final merged file saved as: {final_output}\")\n",
    "print(f\"Total rows collected: {len(final_df)}\")\n",
    "print(\"\\nNo data loss. Full pipeline complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa549c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "BASE_PATH = \"D:\\\\codes\\\\brain tumor project yz\\\\infosys\\\\\"\n",
    "\n",
    "INPUT_FILES = [\n",
    "    \"Recovered_City_chunk_1.csv\",\n",
    "    \"Processed_City_chunk_2.csv\",\n",
    "    \"Recovered_City_chunk_3.csv\",\n",
    "    \"Recovered_City_chunk_4.csv\",\n",
    "    \"Recovered_City_chunk_5.csv\",\n",
    "    \"Processed_City_chunk_6.csv\",\n",
    "    \"Processed_City_chunk_7.csv\",\n",
    "    \"Processed_City_chunk_8.csv\",\n",
    "    \"Processed_City_chunk_9.csv\",\n",
    "    \"Processed_City_chunk_10.csv\"\n",
    "]\n",
    "\n",
    "OUTPUT_FILE = BASE_PATH + \"OSM_FINAL_NO_DUPLICATES.csv\"\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"Road_Count\",\n",
    "    \"Industrial_Count\",\n",
    "    \"Farmland_Count\",\n",
    "    \"Landfill_Count\",\n",
    "    \"Dump_Site_Count\",\n",
    "    \"Recycling_Count\",\n",
    "    \"Green_Area_Count\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "#               LOAD & COMBINE ALL FILES\n",
    "# ============================================================\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in INPUT_FILES:\n",
    "    path = BASE_PATH + file\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        dfs.append(df)\n",
    "        print(f\"‚úî Loaded {file} ({len(df)})\")\n",
    "    else:\n",
    "        print(f\"‚ö† Missing file: {file}\")\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal rows before deduplication: {len(combined_df)}\")\n",
    "\n",
    "# ============================================================\n",
    "#        REMOVE DUPLICATES ‚Äî KEEP BEST ROW PER CITY\n",
    "# ============================================================\n",
    "\n",
    "combined_df[\"feature_sum\"] = combined_df[FEATURE_COLS].sum(axis=1)\n",
    "\n",
    "# Sort so best rows come first\n",
    "combined_df = combined_df.sort_values(\n",
    "    by=[\"City\", \"feature_sum\"],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "# Drop duplicates by City\n",
    "final_df = combined_df.drop_duplicates(subset=\"City\", keep=\"first\")\n",
    "\n",
    "# Cleanup\n",
    "final_df = final_df.drop(columns=[\"feature_sum\"])\n",
    "\n",
    "# ============================================================\n",
    "#                   SAVE FINAL FILE\n",
    "# ============================================================\n",
    "\n",
    "final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\nüéâ FINAL MERGE COMPLETE\")\n",
    "print(f\"Final unique cities: {len(final_df)}\")\n",
    "print(f\"Saved as: {OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
