{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105826e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1754f4f",
   "metadata": {},
   "source": [
    "## Step 1: Load Raw Datasets\n",
    "Load air quality and physical features datasets for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5fa23e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DATASETS\n",
      "======================================================================\n",
      "\n",
      "âœ… Air Quality Dataset loaded: 109,501 records\n",
      "   Columns: ['state', 'district', 'location_id', 'location_name', 'datetime_utc', 'datetime_local', 'latitude', 'longitude', 'pm25', 'pm10', 'no2', 'co', 'so2', 'o3', 'temperature', 'humidity', 'wind_speed', 'wind_direction']\n",
      "\n",
      "âœ… Physical Features Dataset loaded: 49 locations\n",
      "   Columns: ['state', 'district', 'location_id', 'location_name', 'latitude', 'longitude', 'agricultural_area_sqm', 'agricultural_count', 'agricultural_distance_m', 'commercial_area_sqm', 'commercial_count', 'commercial_distance_m', 'dump_sites_area_sqm', 'dump_sites_count', 'dump_sites_distance_m', 'industrial_area_sqm', 'industrial_count', 'industrial_distance_m', 'power_plants_area_sqm', 'power_plants_count', 'power_plants_distance_m', 'residential_area_sqm', 'residential_count', 'residential_distance_m', 'roads_count', 'roads_distance_m', 'roads_total_length_m']\n",
      "\n",
      "--------------------------------------------------\n",
      "Air Quality Data Preview:\n",
      "--------------------------------------------------\n",
      "            state  district  location_id               location_name  \\\n",
      "0  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
      "1  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
      "2  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
      "3  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
      "4  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
      "\n",
      "           datetime_utc             datetime_local  latitude  longitude  pm25  \\\n",
      "0  2025-11-08T09:45:00Z  2025-11-08T15:15:00+05:30     13.67      79.35  25.0   \n",
      "1  2025-11-08T10:00:00Z  2025-11-08T15:30:00+05:30     13.67      79.35  33.0   \n",
      "2  2025-11-08T10:15:00Z  2025-11-08T15:45:00+05:30     13.67      79.35  33.0   \n",
      "3  2025-11-08T10:30:00Z  2025-11-08T16:00:00+05:30     13.67      79.35  33.0   \n",
      "4  2025-11-08T10:45:00Z  2025-11-08T16:15:00+05:30     13.67      79.35  33.0   \n",
      "\n",
      "   pm10   no2    co  so2    o3  temperature  humidity  wind_speed  \\\n",
      "0  63.0  18.6  0.61  4.9  33.0         24.7      58.0         0.3   \n",
      "1  66.0  16.7  0.69  4.9  33.8         24.6      59.0         0.5   \n",
      "2  66.0  19.2  0.54  5.1  31.2         24.5      59.0         0.6   \n",
      "3  66.0  20.6  0.61  5.0  29.2         24.4      60.0         0.5   \n",
      "4  66.0  18.9  0.68  5.0  29.7         24.3      60.0         0.3   \n",
      "\n",
      "   wind_direction  \n",
      "0           355.0  \n",
      "1           355.0  \n",
      "2           355.0  \n",
      "3           355.0  \n",
      "4           355.0  \n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Datasets\n",
    "\n",
    "# File paths\n",
    "aq_file = \"data/processed/india_aq_transformed_last30days.csv\"\n",
    "pf_file = \"data/processed/india_physical_features.csv\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load Air Quality Data\n",
    "if os.path.exists(aq_file):\n",
    "    aq_df = pd.read_csv(aq_file)\n",
    "    print(f\"\\nâœ… Air Quality Dataset loaded: {len(aq_df):,} records\")\n",
    "    print(f\"   Columns: {list(aq_df.columns)}\")\n",
    "else:\n",
    "    print(f\"âŒ File not found: {aq_file}\")\n",
    "    aq_df = pd.DataFrame()\n",
    "\n",
    "# Load Physical Features Data\n",
    "if os.path.exists(pf_file):\n",
    "    pf_df = pd.read_csv(pf_file)\n",
    "    print(f\"\\nâœ… Physical Features Dataset loaded: {len(pf_df)} locations\")\n",
    "    print(f\"   Columns: {list(pf_df.columns)}\")\n",
    "else:\n",
    "    print(f\"âŒ File not found: {pf_file}\")\n",
    "    pf_df = pd.DataFrame()\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Air Quality Data Preview:\")\n",
    "print(\"-\"*50)\n",
    "print(aq_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9891c",
   "metadata": {},
   "source": [
    "## Step 2: Remove Duplicates and Invalid Records\n",
    "Clean the dataset by removing duplicate entries and invalid records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9bda2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REMOVING DUPLICATES AND INVALID RECORDS\n",
      "======================================================================\n",
      "\n",
      "Initial records: 109,501\n",
      "After removing duplicates: 109,501 (removed 0)\n",
      "After removing invalid coordinates: 109,501 (removed 0)\n",
      "After removing missing timestamps: 109,501 (removed 0)\n",
      "After removing all-null pollutant records: 109,425 (removed 76)\n",
      "After removing outliers: 106,369 (removed 3,056)\n",
      "\n",
      "ðŸ“Š Total records removed: 3,132\n",
      "ðŸ“Š Final clean records: 106,369\n",
      "ðŸ“Š Data retention: 97.14%\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Remove Duplicates and Invalid Records\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REMOVING DUPLICATES AND INVALID RECORDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "initial_count = len(aq_df)\n",
    "print(f\"\\nInitial records: {initial_count:,}\")\n",
    "\n",
    "# Remove exact duplicates\n",
    "aq_df = aq_df.drop_duplicates()\n",
    "after_dup = len(aq_df)\n",
    "print(f\"After removing duplicates: {after_dup:,} (removed {initial_count - after_dup:,})\")\n",
    "\n",
    "# Remove records with invalid coordinates\n",
    "aq_df = aq_df[(aq_df['latitude'].notna()) & (aq_df['longitude'].notna())]\n",
    "aq_df = aq_df[(aq_df['latitude'] >= -90) & (aq_df['latitude'] <= 90)]\n",
    "aq_df = aq_df[(aq_df['longitude'] >= -180) & (aq_df['longitude'] <= 180)]\n",
    "after_coords = len(aq_df)\n",
    "print(f\"After removing invalid coordinates: {after_coords:,} (removed {after_dup - after_coords:,})\")\n",
    "\n",
    "# Remove records with missing timestamps\n",
    "aq_df = aq_df[aq_df['datetime_utc'].notna()]\n",
    "after_time = len(aq_df)\n",
    "print(f\"After removing missing timestamps: {after_time:,} (removed {after_coords - after_time:,})\")\n",
    "\n",
    "# Remove records where ALL pollutant values are missing\n",
    "pollutant_cols = ['pm25', 'pm10', 'no2', 'co', 'so2', 'o3']\n",
    "existing_pollutants = [col for col in pollutant_cols if col in aq_df.columns]\n",
    "aq_df = aq_df.dropna(subset=existing_pollutants, how='all')\n",
    "after_pollutants = len(aq_df)\n",
    "print(f\"After removing all-null pollutant records: {after_pollutants:,} (removed {after_time - after_pollutants:,})\")\n",
    "\n",
    "# Remove outliers (values outside 3 standard deviations)\n",
    "for col in existing_pollutants:\n",
    "    if col in aq_df.columns:\n",
    "        mean_val = aq_df[col].mean()\n",
    "        std_val = aq_df[col].std()\n",
    "        lower_bound = mean_val - 3 * std_val\n",
    "        upper_bound = mean_val + 3 * std_val\n",
    "        # Only filter if values are unrealistic (negative or extremely high)\n",
    "        aq_df = aq_df[(aq_df[col].isna()) | ((aq_df[col] >= 0) & (aq_df[col] <= upper_bound))]\n",
    "\n",
    "final_count = len(aq_df)\n",
    "print(f\"After removing outliers: {final_count:,} (removed {after_pollutants - final_count:,})\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total records removed: {initial_count - final_count:,}\")\n",
    "print(f\"ðŸ“Š Final clean records: {final_count:,}\")\n",
    "print(f\"ðŸ“Š Data retention: {(final_count/initial_count)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea0a7d7",
   "metadata": {},
   "source": [
    "## Step 3: Handle Missing Values\n",
    "Impute missing values using mean/median for pollutants and forward-fill for weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec18215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HANDLING MISSING VALUES\n",
      "======================================================================\n",
      "\n",
      "Missing values BEFORE imputation:\n",
      "--------------------------------------------------\n",
      "  pm25                     : 3,564 missing (3.35%)\n",
      "  pm10                     : 8,245 missing (7.75%)\n",
      "  no2                      : 3,493 missing (3.28%)\n",
      "  co                       : 8,353 missing (7.85%)\n",
      "  so2                      : 2,512 missing (2.36%)\n",
      "  o3                       : 10,152 missing (9.54%)\n",
      "  temperature              : 23,516 missing (22.11%)\n",
      "  humidity                 : 15,882 missing (14.93%)\n",
      "  wind_speed               : 20,271 missing (19.06%)\n",
      "  wind_direction           : 17,946 missing (16.87%)\n",
      "\n",
      "Imputing pollutant values with location-wise median...\n",
      "Imputing weather values with location-wise mean...\n",
      "\n",
      "Missing values AFTER imputation:\n",
      "--------------------------------------------------\n",
      "  âœ… No missing values in numeric columns!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Handle Missing Values\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check missing values before imputation\n",
    "print(\"\\nMissing values BEFORE imputation:\")\n",
    "print(\"-\"*50)\n",
    "missing_before = aq_df.isnull().sum()\n",
    "for col in aq_df.columns:\n",
    "    missing = missing_before[col]\n",
    "    pct = (missing / len(aq_df)) * 100\n",
    "    if missing > 0:\n",
    "        print(f\"  {col:25s}: {missing:,} missing ({pct:.2f}%)\")\n",
    "\n",
    "# Define column categories\n",
    "pollutant_cols = ['pm25', 'pm10', 'no2', 'co', 'so2', 'o3']\n",
    "weather_cols = ['temperature', 'humidity', 'wind_speed', 'wind_direction']\n",
    "\n",
    "# Impute pollutants with median (robust to outliers) grouped by location\n",
    "print(\"\\nImputing pollutant values with location-wise median...\")\n",
    "for col in pollutant_cols:\n",
    "    if col in aq_df.columns:\n",
    "        # Group by location and fill with median\n",
    "        aq_df[col] = aq_df.groupby('location_id')[col].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "        # Fill remaining with global median\n",
    "        aq_df[col] = aq_df[col].fillna(aq_df[col].median())\n",
    "\n",
    "# Impute weather data with location-wise mean\n",
    "print(\"Imputing weather values with location-wise mean...\")\n",
    "for col in weather_cols:\n",
    "    if col in aq_df.columns:\n",
    "        aq_df[col] = aq_df.groupby('location_id')[col].transform(\n",
    "            lambda x: x.fillna(x.mean())\n",
    "        )\n",
    "        # Fill remaining with global mean\n",
    "        aq_df[col] = aq_df[col].fillna(aq_df[col].mean())\n",
    "\n",
    "# Check missing values after imputation\n",
    "print(\"\\nMissing values AFTER imputation:\")\n",
    "print(\"-\"*50)\n",
    "missing_after = aq_df.isnull().sum()\n",
    "for col in aq_df.columns:\n",
    "    missing = missing_after[col]\n",
    "    if missing > 0:\n",
    "        print(f\"  {col:25s}: {missing:,} missing\")\n",
    "\n",
    "if missing_after.sum() == 0:\n",
    "    print(\"  âœ… No missing values in numeric columns!\")\n",
    "else:\n",
    "    print(f\"  âš  Total remaining missing: {missing_after.sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c28094",
   "metadata": {},
   "source": [
    "## Step 4: Standardize Timestamps\n",
    "Parse datetime strings and extract temporal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bfdf1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STANDARDIZING TIMESTAMPS\n",
      "======================================================================\n",
      "\n",
      "Converting datetime strings to datetime objects...\n",
      "Extracting date components...\n",
      "Standardizing GPS coordinates...\n",
      "\n",
      "âœ… Timestamp standardization complete!\n",
      "\n",
      "Date range: 2025-11-08 to 2025-12-08\n",
      "Hours covered: [np.int32(0), np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6), np.int32(7), np.int32(8), np.int32(9), np.int32(10), np.int32(11), np.int32(12), np.int32(13), np.int32(14), np.int32(15), np.int32(16), np.int32(17), np.int32(18), np.int32(19), np.int32(20), np.int32(21), np.int32(22), np.int32(23)]\n",
      "\n",
      "--------------------------------------------------\n",
      "Sample of standardized data:\n",
      "--------------------------------------------------\n",
      "               datetime_ist        date  hour  day_of_week  is_weekend\n",
      "0 2025-11-08 15:15:00+05:30  2025-11-08    15            5           1\n",
      "1 2025-11-08 15:30:00+05:30  2025-11-08    15            5           1\n",
      "2 2025-11-08 15:45:00+05:30  2025-11-08    15            5           1\n",
      "3 2025-11-08 16:00:00+05:30  2025-11-08    16            5           1\n",
      "4 2025-11-08 16:15:00+05:30  2025-11-08    16            5           1\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Standardize Timestamps\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STANDARDIZING TIMESTAMPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert datetime_utc to datetime object\n",
    "print(\"\\nConverting datetime strings to datetime objects...\")\n",
    "aq_df['datetime_utc'] = pd.to_datetime(aq_df['datetime_utc'], utc=True)\n",
    "\n",
    "# Convert to IST (Indian Standard Time) for local analysis\n",
    "aq_df['datetime_ist'] = aq_df['datetime_utc'].dt.tz_convert('Asia/Kolkata')\n",
    "\n",
    "# Extract date components\n",
    "print(\"Extracting date components...\")\n",
    "aq_df['date'] = aq_df['datetime_ist'].dt.date\n",
    "aq_df['year'] = aq_df['datetime_ist'].dt.year\n",
    "aq_df['month'] = aq_df['datetime_ist'].dt.month\n",
    "aq_df['day'] = aq_df['datetime_ist'].dt.day\n",
    "aq_df['hour'] = aq_df['datetime_ist'].dt.hour\n",
    "aq_df['day_of_week'] = aq_df['datetime_ist'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "aq_df['day_name'] = aq_df['datetime_ist'].dt.day_name()\n",
    "aq_df['is_weekend'] = aq_df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Round coordinates to standard precision\n",
    "print(\"Standardizing GPS coordinates...\")\n",
    "aq_df['latitude'] = aq_df['latitude'].round(6)\n",
    "aq_df['longitude'] = aq_df['longitude'].round(6)\n",
    "\n",
    "print(\"\\nâœ… Timestamp standardization complete!\")\n",
    "print(f\"\\nDate range: {aq_df['date'].min()} to {aq_df['date'].max()}\")\n",
    "print(f\"Hours covered: {sorted(aq_df['hour'].unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Sample of standardized data:\")\n",
    "print(\"-\"*50)\n",
    "print(aq_df[['datetime_ist', 'date', 'hour', 'day_of_week', 'is_weekend']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403023a5",
   "metadata": {},
   "source": [
    "## Step 5: Derive Temporal Features\n",
    "Create time-based features for pollution pattern analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "331e3bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DERIVING TEMPORAL FEATURES\n",
      "======================================================================\n",
      "\n",
      "Creating time of day categories...\n",
      "Creating season categories...\n",
      "Creating rush hour indicator...\n",
      "Creating cyclical time encoding...\n",
      "\n",
      "âœ… Temporal features created!\n",
      "\n",
      "--------------------------------------------------\n",
      "Temporal Feature Distribution:\n",
      "--------------------------------------------------\n",
      "\n",
      "Time of Day:\n",
      "time_of_day\n",
      "night            34780\n",
      "afternoon        20510\n",
      "early_morning    19480\n",
      "evening          18251\n",
      "morning          13348\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Season:\n",
      "season\n",
      "post_monsoon    78351\n",
      "winter          28018\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Weekend vs Weekday: Weekend=34,339, Weekday=72,030\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Derive Temporal Features\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DERIVING TEMPORAL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Time of day categories\n",
    "def get_time_of_day(hour):\n",
    "    if 5 <= hour < 9:\n",
    "        return 'early_morning'\n",
    "    elif 9 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "print(\"\\nCreating time of day categories...\")\n",
    "aq_df['time_of_day'] = aq_df['hour'].apply(get_time_of_day)\n",
    "\n",
    "# Season based on month (for India)\n",
    "def get_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'summer'\n",
    "    elif month in [6, 7, 8, 9]:\n",
    "        return 'monsoon'\n",
    "    elif month in [10, 11]:\n",
    "        return 'post_monsoon'\n",
    "    else:  # 12, 1, 2\n",
    "        return 'winter'\n",
    "\n",
    "print(\"Creating season categories...\")\n",
    "aq_df['season'] = aq_df['month'].apply(get_season)\n",
    "\n",
    "# Rush hour indicator (traffic-related pollution peaks)\n",
    "print(\"Creating rush hour indicator...\")\n",
    "aq_df['is_rush_hour'] = aq_df['hour'].isin([8, 9, 10, 17, 18, 19, 20]).astype(int)\n",
    "\n",
    "# Cyclical encoding for hour (captures circular nature of time)\n",
    "print(\"Creating cyclical time encoding...\")\n",
    "aq_df['hour_sin'] = np.sin(2 * np.pi * aq_df['hour'] / 24)\n",
    "aq_df['hour_cos'] = np.cos(2 * np.pi * aq_df['hour'] / 24)\n",
    "aq_df['day_sin'] = np.sin(2 * np.pi * aq_df['day_of_week'] / 7)\n",
    "aq_df['day_cos'] = np.cos(2 * np.pi * aq_df['day_of_week'] / 7)\n",
    "aq_df['month_sin'] = np.sin(2 * np.pi * aq_df['month'] / 12)\n",
    "aq_df['month_cos'] = np.cos(2 * np.pi * aq_df['month'] / 12)\n",
    "\n",
    "print(\"\\nâœ… Temporal features created!\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Temporal Feature Distribution:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"\\nTime of Day:\\n{aq_df['time_of_day'].value_counts()}\")\n",
    "print(f\"\\nSeason:\\n{aq_df['season'].value_counts()}\")\n",
    "print(f\"\\nWeekend vs Weekday: Weekend={aq_df['is_weekend'].sum():,}, Weekday={len(aq_df) - aq_df['is_weekend'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450d81f",
   "metadata": {},
   "source": [
    "## Step 6: Merge with Physical Features\n",
    "Combine air quality data with location-based physical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1872e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MERGING WITH PHYSICAL FEATURES\n",
      "======================================================================\n",
      "\n",
      "Air Quality records: 106,369\n",
      "Physical Features locations: 49\n",
      "\n",
      "Physical Features columns:\n",
      "['state', 'district', 'location_id', 'location_name', 'latitude', 'longitude', 'agricultural_area_sqm', 'agricultural_count', 'agricultural_distance_m', 'commercial_area_sqm', 'commercial_count', 'commercial_distance_m', 'dump_sites_area_sqm', 'dump_sites_count', 'dump_sites_distance_m', 'industrial_area_sqm', 'industrial_count', 'industrial_distance_m', 'power_plants_area_sqm', 'power_plants_count', 'power_plants_distance_m', 'residential_area_sqm', 'residential_count', 'residential_distance_m', 'roads_count', 'roads_distance_m', 'roads_total_length_m']\n",
      "\n",
      "Merging on 'location_id'...\n",
      "\n",
      "âœ… Merge complete!\n",
      "   Merged records: 106,369\n",
      "   Total columns: 57\n",
      "   âœ… All locations matched with physical features!\n",
      "\n",
      "--------------------------------------------------\n",
      "Merged Dataset Preview:\n",
      "--------------------------------------------------\n",
      "                location_name  pm25  roads_count  industrial_count  \\\n",
      "0  Tirumala, Tirupati - APPCB  25.0           67                 2   \n",
      "1  Tirumala, Tirupati - APPCB  33.0           67                 2   \n",
      "2  Tirumala, Tirupati - APPCB  33.0           67                 2   \n",
      "3  Tirumala, Tirupati - APPCB  33.0           67                 2   \n",
      "4  Tirumala, Tirupati - APPCB  33.0           67                 2   \n",
      "\n",
      "   agricultural_count  \n",
      "0                   0  \n",
      "1                   0  \n",
      "2                   0  \n",
      "3                   0  \n",
      "4                   0  \n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Merge with Physical Features Dataset\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MERGING WITH PHYSICAL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nAir Quality records: {len(aq_df):,}\")\n",
    "print(f\"Physical Features locations: {len(pf_df)}\")\n",
    "\n",
    "# Check common columns\n",
    "print(\"\\nPhysical Features columns:\")\n",
    "print(pf_df.columns.tolist())\n",
    "\n",
    "# Merge on location_id\n",
    "# Select relevant columns from physical features (excluding duplicate location info)\n",
    "pf_cols_to_merge = [col for col in pf_df.columns if col not in \n",
    "                    ['state', 'district', 'location_name', 'latitude', 'longitude']]\n",
    "\n",
    "print(f\"\\nMerging on 'location_id'...\")\n",
    "merged_df = pd.merge(\n",
    "    aq_df,\n",
    "    pf_df[pf_cols_to_merge],\n",
    "    on='location_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Merge complete!\")\n",
    "print(f\"   Merged records: {len(merged_df):,}\")\n",
    "print(f\"   Total columns: {len(merged_df.columns)}\")\n",
    "\n",
    "# Check for unmatched records\n",
    "unmatched = merged_df[merged_df['roads_count'].isna()]['location_id'].nunique()\n",
    "if unmatched > 0:\n",
    "    print(f\"   âš  Locations without physical features: {unmatched}\")\n",
    "else:\n",
    "    print(f\"   âœ… All locations matched with physical features!\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Merged Dataset Preview:\")\n",
    "print(\"-\"*50)\n",
    "print(merged_df[['location_name', 'pm25', 'roads_count', 'industrial_count', 'agricultural_count']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a113729",
   "metadata": {},
   "source": [
    "## Step 7: Calculate Spatial Proximity Features\n",
    "Create distance-based features for pollution source classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e2a251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CALCULATING SPATIAL PROXIMITY FEATURES\n",
      "======================================================================\n",
      "\n",
      "Distance columns found: ['agricultural_distance_m', 'commercial_distance_m', 'dump_sites_distance_m', 'industrial_distance_m', 'power_plants_distance_m', 'residential_distance_m', 'roads_distance_m']\n",
      "\n",
      "Creating proximity category features...\n",
      "  âœ“ road_proximity created\n",
      "  âœ“ industrial_proximity created\n",
      "  âœ“ agricultural_proximity created\n",
      "  âœ“ dump_proximity created\n",
      "\n",
      "Creating binary proximity flags (within 1km)...\n",
      "\n",
      "âœ… Spatial proximity features created!\n",
      "\n",
      "--------------------------------------------------\n",
      "Proximity Distribution:\n",
      "--------------------------------------------------\n",
      "\n",
      "Road Proximity:\n",
      "road_proximity\n",
      "near      78984\n",
      "medium    18937\n",
      "far        8448\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Calculate Spatial Proximity Features\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CALCULATING SPATIAL PROXIMITY FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Distance columns from physical features\n",
    "distance_cols = [col for col in merged_df.columns if '_distance_m' in col]\n",
    "print(f\"\\nDistance columns found: {distance_cols}\")\n",
    "\n",
    "# Create proximity categories based on distances\n",
    "print(\"\\nCreating proximity category features...\")\n",
    "\n",
    "def categorize_distance(distance, near=500, medium=1000):\n",
    "    \"\"\"Categorize distance as near/medium/far\"\"\"\n",
    "    if pd.isna(distance):\n",
    "        return 'unknown'\n",
    "    elif distance <= near:\n",
    "        return 'near'\n",
    "    elif distance <= medium:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'far'\n",
    "\n",
    "# Create categorical proximity features\n",
    "if 'roads_distance_m' in merged_df.columns:\n",
    "    merged_df['road_proximity'] = merged_df['roads_distance_m'].apply(\n",
    "        lambda x: categorize_distance(x, near=200, medium=500))\n",
    "    print(\"  âœ“ road_proximity created\")\n",
    "\n",
    "if 'industrial_distance_m' in merged_df.columns:\n",
    "    merged_df['industrial_proximity'] = merged_df['industrial_distance_m'].apply(\n",
    "        lambda x: categorize_distance(x, near=500, medium=1500))\n",
    "    print(\"  âœ“ industrial_proximity created\")\n",
    "\n",
    "if 'agricultural_distance_m' in merged_df.columns:\n",
    "    merged_df['agricultural_proximity'] = merged_df['agricultural_distance_m'].apply(\n",
    "        lambda x: categorize_distance(x, near=500, medium=1500))\n",
    "    print(\"  âœ“ agricultural_proximity created\")\n",
    "\n",
    "if 'dump_sites_distance_m' in merged_df.columns:\n",
    "    merged_df['dump_proximity'] = merged_df['dump_sites_distance_m'].apply(\n",
    "        lambda x: categorize_distance(x, near=500, medium=1500))\n",
    "    print(\"  âœ“ dump_proximity created\")\n",
    "\n",
    "# Create binary proximity flags (within 1km)\n",
    "print(\"\\nCreating binary proximity flags (within 1km)...\")\n",
    "if 'roads_distance_m' in merged_df.columns:\n",
    "    merged_df['near_road'] = (merged_df['roads_distance_m'] <= 500).astype(int)\n",
    "if 'industrial_distance_m' in merged_df.columns:\n",
    "    merged_df['near_industry'] = (merged_df['industrial_distance_m'] <= 1000).astype(int)\n",
    "if 'agricultural_distance_m' in merged_df.columns:\n",
    "    merged_df['near_agriculture'] = (merged_df['agricultural_distance_m'] <= 1000).astype(int)\n",
    "if 'dump_sites_distance_m' in merged_df.columns:\n",
    "    merged_df['near_dump'] = (merged_df['dump_sites_distance_m'] <= 1000).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Spatial proximity features created!\")\n",
    "\n",
    "# Show proximity distribution\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Proximity Distribution:\")\n",
    "print(\"-\"*50)\n",
    "if 'road_proximity' in merged_df.columns:\n",
    "    print(f\"\\nRoad Proximity:\\n{merged_df['road_proximity'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d359ec9",
   "metadata": {},
   "source": [
    "## Step 8: Normalize Pollutant and Weather Values\n",
    "Scale numeric features for consistent model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a96e1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NORMALIZING NUMERIC FEATURES\n",
      "======================================================================\n",
      "\n",
      "Columns to normalize: ['pm25', 'pm10', 'no2', 'co', 'so2', 'o3', 'temperature', 'humidity', 'wind_speed', 'wind_direction']\n",
      "\n",
      "Applying Min-Max normalization (0-1 range)...\n",
      "  âœ“ pm25_normalized created\n",
      "  âœ“ pm10_normalized created\n",
      "  âœ“ no2_normalized created\n",
      "  âœ“ co_normalized created\n",
      "  âœ“ so2_normalized created\n",
      "  âœ“ o3_normalized created\n",
      "  âœ“ temperature_normalized created\n",
      "  âœ“ humidity_normalized created\n",
      "  âœ“ wind_speed_normalized created\n",
      "  âœ“ wind_direction_normalized created\n",
      "\n",
      "Applying Standard scaling (z-score)...\n",
      "  âœ“ pm25_scaled created\n",
      "  âœ“ pm10_scaled created\n",
      "  âœ“ no2_scaled created\n",
      "  âœ“ co_scaled created\n",
      "  âœ“ so2_scaled created\n",
      "  âœ“ o3_scaled created\n",
      "  âœ“ temperature_scaled created\n",
      "  âœ“ humidity_scaled created\n",
      "  âœ“ wind_speed_scaled created\n",
      "  âœ“ wind_direction_scaled created\n",
      "\n",
      "âœ… Normalization complete!\n",
      "\n",
      "--------------------------------------------------\n",
      "Sample Normalized Values:\n",
      "--------------------------------------------------\n",
      "                pm25  pm25_normalized   pm25_scaled\n",
      "count  106369.000000    106369.000000  1.063690e+05\n",
      "mean       62.171126         0.004698  2.778871e-17\n",
      "std       111.535896         0.008429  1.000005e+00\n",
      "min         0.000000         0.000000 -5.574117e-01\n",
      "25%        29.680000         0.002243 -2.913078e-01\n",
      "50%        51.820000         0.003916 -9.280577e-02\n",
      "75%        77.340000         0.005845  1.360006e-01\n",
      "max     13232.170000         1.000000  1.180791e+02\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Normalize Pollutant and Weather Values\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NORMALIZING NUMERIC FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define columns to normalize\n",
    "pollutant_cols = ['pm25', 'pm10', 'no2', 'co', 'so2', 'o3']\n",
    "weather_cols = ['temperature', 'humidity', 'wind_speed', 'wind_direction']\n",
    "\n",
    "# Get existing columns\n",
    "cols_to_normalize = [col for col in pollutant_cols + weather_cols if col in merged_df.columns]\n",
    "print(f\"\\nColumns to normalize: {cols_to_normalize}\")\n",
    "\n",
    "# Create normalized versions using Min-Max scaling (0-1)\n",
    "print(\"\\nApplying Min-Max normalization (0-1 range)...\")\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for col in cols_to_normalize:\n",
    "    col_data = merged_df[col].values.reshape(-1, 1)\n",
    "    merged_df[f'{col}_normalized'] = scaler.fit_transform(col_data)\n",
    "    print(f\"  âœ“ {col}_normalized created\")\n",
    "\n",
    "# Also create standardized versions (mean=0, std=1) for comparison\n",
    "print(\"\\nApplying Standard scaling (z-score)...\")\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "for col in cols_to_normalize:\n",
    "    col_data = merged_df[col].values.reshape(-1, 1)\n",
    "    merged_df[f'{col}_scaled'] = std_scaler.fit_transform(col_data)\n",
    "    print(f\"  âœ“ {col}_scaled created\")\n",
    "\n",
    "print(\"\\nâœ… Normalization complete!\")\n",
    "\n",
    "# Show sample of normalized data\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Sample Normalized Values:\")\n",
    "print(\"-\"*50)\n",
    "sample_cols = ['pm25', 'pm25_normalized', 'pm25_scaled'] if 'pm25' in merged_df.columns else []\n",
    "if sample_cols:\n",
    "    print(merged_df[sample_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac4773",
   "metadata": {},
   "source": [
    "## Step 9: Create Final Feature Set\n",
    "Organize all features into a clean, unified DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec66039b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CREATING FINAL UNIFIED DATASET\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Final Dataset Summary:\n",
      "   Total records: 106,369\n",
      "   Total features: 84\n",
      "   Unique locations: 49\n",
      "   Date range: 2025-11-08 to 2025-12-08\n",
      "\n",
      "--------------------------------------------------\n",
      "Feature Categories:\n",
      "--------------------------------------------------\n",
      "  Identity columns: 6\n",
      "  Temporal columns: 19\n",
      "  Pollutant columns: 6\n",
      "  Weather columns: 4\n",
      "  Physical feature columns: 21\n",
      "  Proximity columns: 8\n",
      "  Normalized columns: 20\n",
      "\n",
      "--------------------------------------------------\n",
      "Final Dataset Preview:\n",
      "--------------------------------------------------\n",
      "            state  district  location_id               location_name  \\\n",
      "0  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
      "1  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
      "2  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
      "3  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
      "4  Andhra Pradesh  Tirupati         5649  Tirumala, Tirupati - APPCB   \n",
      "\n",
      "   latitude  longitude              datetime_utc              datetime_ist  \\\n",
      "0     13.67      79.35 2025-11-08 09:45:00+00:00 2025-11-08 15:15:00+05:30   \n",
      "1     13.67      79.35 2025-11-08 10:00:00+00:00 2025-11-08 15:30:00+05:30   \n",
      "2     13.67      79.35 2025-11-08 10:15:00+00:00 2025-11-08 15:45:00+05:30   \n",
      "3     13.67      79.35 2025-11-08 10:30:00+00:00 2025-11-08 16:00:00+05:30   \n",
      "4     13.67      79.35 2025-11-08 10:45:00+00:00 2025-11-08 16:15:00+05:30   \n",
      "\n",
      "         date  year  month  day  hour  day_of_week  day_name  is_weekend  \\\n",
      "0  2025-11-08  2025     11    8    15            5  Saturday           1   \n",
      "1  2025-11-08  2025     11    8    15            5  Saturday           1   \n",
      "2  2025-11-08  2025     11    8    15            5  Saturday           1   \n",
      "3  2025-11-08  2025     11    8    16            5  Saturday           1   \n",
      "4  2025-11-08  2025     11    8    16            5  Saturday           1   \n",
      "\n",
      "  time_of_day        season  is_rush_hour  hour_sin  hour_cos   day_sin  \\\n",
      "0   afternoon  post_monsoon             0 -0.707107 -0.707107 -0.974928   \n",
      "1   afternoon  post_monsoon             0 -0.707107 -0.707107 -0.974928   \n",
      "2   afternoon  post_monsoon             0 -0.707107 -0.707107 -0.974928   \n",
      "3   afternoon  post_monsoon             0 -0.866025 -0.500000 -0.974928   \n",
      "4   afternoon  post_monsoon             0 -0.866025 -0.500000 -0.974928   \n",
      "\n",
      "    day_cos  month_sin  month_cos  pm25  pm10   no2    co  so2    o3  \\\n",
      "0 -0.222521       -0.5   0.866025  25.0  63.0  18.6  0.61  4.9  33.0   \n",
      "1 -0.222521       -0.5   0.866025  33.0  66.0  16.7  0.69  4.9  33.8   \n",
      "2 -0.222521       -0.5   0.866025  33.0  66.0  19.2  0.54  5.1  31.2   \n",
      "3 -0.222521       -0.5   0.866025  33.0  66.0  20.6  0.61  5.0  29.2   \n",
      "4 -0.222521       -0.5   0.866025  33.0  66.0  18.9  0.68  5.0  29.7   \n",
      "\n",
      "   temperature  humidity  wind_speed  wind_direction  agricultural_area_sqm  \\\n",
      "0         24.7      58.0         0.3           355.0                    0.0   \n",
      "1         24.6      59.0         0.5           355.0                    0.0   \n",
      "2         24.5      59.0         0.6           355.0                    0.0   \n",
      "3         24.4      60.0         0.5           355.0                    0.0   \n",
      "4         24.3      60.0         0.3           355.0                    0.0   \n",
      "\n",
      "   agricultural_count  agricultural_distance_m  commercial_area_sqm  \\\n",
      "0                   0                      NaN           1533014.61   \n",
      "1                   0                      NaN           1533014.61   \n",
      "2                   0                      NaN           1533014.61   \n",
      "3                   0                      NaN           1533014.61   \n",
      "4                   0                      NaN           1533014.61   \n",
      "\n",
      "   commercial_count  commercial_distance_m  dump_sites_area_sqm  \\\n",
      "0                10                 445.54             47901.32   \n",
      "1                10                 445.54             47901.32   \n",
      "2                10                 445.54             47901.32   \n",
      "3                10                 445.54             47901.32   \n",
      "4                10                 445.54             47901.32   \n",
      "\n",
      "   dump_sites_count  dump_sites_distance_m  industrial_area_sqm  \\\n",
      "0                 3                 976.01              7470.18   \n",
      "1                 3                 976.01              7470.18   \n",
      "2                 3                 976.01              7470.18   \n",
      "3                 3                 976.01              7470.18   \n",
      "4                 3                 976.01              7470.18   \n",
      "\n",
      "   industrial_count  industrial_distance_m  power_plants_area_sqm  \\\n",
      "0                 2                  396.9                    0.0   \n",
      "1                 2                  396.9                    0.0   \n",
      "2                 2                  396.9                    0.0   \n",
      "3                 2                  396.9                    0.0   \n",
      "4                 2                  396.9                    0.0   \n",
      "\n",
      "   power_plants_count  power_plants_distance_m  residential_area_sqm  \\\n",
      "0                   2                   352.91             218283.09   \n",
      "1                   2                   352.91             218283.09   \n",
      "2                   2                   352.91             218283.09   \n",
      "3                   2                   352.91             218283.09   \n",
      "4                   2                   352.91             218283.09   \n",
      "\n",
      "   residential_count  residential_distance_m  roads_count  roads_distance_m  \\\n",
      "0                 15                   213.7           67             63.78   \n",
      "1                 15                   213.7           67             63.78   \n",
      "2                 15                   213.7           67             63.78   \n",
      "3                 15                   213.7           67             63.78   \n",
      "4                 15                   213.7           67             63.78   \n",
      "\n",
      "   roads_total_length_m road_proximity industrial_proximity  \\\n",
      "0              52878.48           near                 near   \n",
      "1              52878.48           near                 near   \n",
      "2              52878.48           near                 near   \n",
      "3              52878.48           near                 near   \n",
      "4              52878.48           near                 near   \n",
      "\n",
      "  agricultural_proximity dump_proximity  near_road  near_industry  \\\n",
      "0                unknown         medium          1              1   \n",
      "1                unknown         medium          1              1   \n",
      "2                unknown         medium          1              1   \n",
      "3                unknown         medium          1              1   \n",
      "4                unknown         medium          1              1   \n",
      "\n",
      "   near_agriculture  near_dump  pm25_normalized  pm10_normalized  \\\n",
      "0                 0          1         0.001889         0.074680   \n",
      "1                 0          1         0.002494         0.078236   \n",
      "2                 0          1         0.002494         0.078236   \n",
      "3                 0          1         0.002494         0.078236   \n",
      "4                 0          1         0.002494         0.078236   \n",
      "\n",
      "   no2_normalized  co_normalized  so2_normalized  o3_normalized  \\\n",
      "0        0.061040       0.204698        0.027283       0.013848   \n",
      "1        0.054804       0.231544        0.027283       0.014184   \n",
      "2        0.063009       0.181208        0.028396       0.013093   \n",
      "3        0.067603       0.204698        0.027840       0.012253   \n",
      "4        0.062024       0.228188        0.027840       0.012463   \n",
      "\n",
      "   temperature_normalized  humidity_normalized  wind_speed_normalized  \\\n",
      "0                0.542394             0.508237               0.005528   \n",
      "1                0.541687             0.514079               0.007002   \n",
      "2                0.540980             0.514079               0.007739   \n",
      "3                0.540273             0.519921               0.007002   \n",
      "4                0.539566             0.519921               0.005528   \n",
      "\n",
      "   wind_direction_normalized  pm25_scaled  pm10_scaled  no2_scaled  co_scaled  \\\n",
      "0                   0.986191    -0.333268    -0.648913   -0.099197  -0.307032   \n",
      "1                   0.986191    -0.261541    -0.611752   -0.180144  -0.162807   \n",
      "2                   0.986191    -0.261541    -0.611752   -0.073635  -0.433230   \n",
      "3                   0.986191    -0.261541    -0.611752   -0.013990  -0.307032   \n",
      "4                   0.986191    -0.261541    -0.611752   -0.086416  -0.180835   \n",
      "\n",
      "   so2_scaled  o3_scaled  temperature_scaled  humidity_scaled  \\\n",
      "0   -0.601595   0.119301            0.561527        -0.172062   \n",
      "1   -0.601595   0.144270            0.546976        -0.131648   \n",
      "2   -0.588860   0.063120            0.532425        -0.131648   \n",
      "3   -0.595227   0.000697            0.517874        -0.091235   \n",
      "4   -0.595227   0.016303            0.503323        -0.091235   \n",
      "\n",
      "   wind_speed_scaled  wind_direction_scaled  \n",
      "0          -0.228994               1.964098  \n",
      "1          -0.216033               1.964098  \n",
      "2          -0.209553               1.964098  \n",
      "3          -0.216033               1.964098  \n",
      "4          -0.228994               1.964098  \n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Create Final Unified DataFrame\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING FINAL UNIFIED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define column groups for final dataset\n",
    "id_cols = ['state', 'district', 'location_id', 'location_name', 'latitude', 'longitude']\n",
    "time_cols = ['datetime_utc', 'datetime_ist', 'date', 'year', 'month', 'day', 'hour', \n",
    "             'day_of_week', 'day_name', 'is_weekend', 'time_of_day', 'season', 'is_rush_hour']\n",
    "cyclical_cols = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos']\n",
    "pollutant_cols = ['pm25', 'pm10', 'no2', 'co', 'so2', 'o3']\n",
    "weather_cols = ['temperature', 'humidity', 'wind_speed', 'wind_direction']\n",
    "\n",
    "# Physical feature columns\n",
    "physical_feature_cols = [col for col in merged_df.columns if any(x in col for x in \n",
    "    ['_count', '_distance_m', '_area_sqm', '_total_length_m'])]\n",
    "\n",
    "# Proximity columns\n",
    "proximity_cols = [col for col in merged_df.columns if '_proximity' in col or col.startswith('near_')]\n",
    "\n",
    "# Normalized columns\n",
    "normalized_cols = [col for col in merged_df.columns if '_normalized' in col or '_scaled' in col]\n",
    "\n",
    "# Get all columns that exist\n",
    "all_cols = (id_cols + time_cols + cyclical_cols + pollutant_cols + weather_cols + \n",
    "            physical_feature_cols + proximity_cols + normalized_cols)\n",
    "final_cols = [col for col in all_cols if col in merged_df.columns]\n",
    "\n",
    "# Create final DataFrame\n",
    "final_df = merged_df[final_cols].copy()\n",
    "\n",
    "# Sort by location and datetime\n",
    "final_df = final_df.sort_values(['state', 'district', 'location_name', 'datetime_ist'])\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Dataset Summary:\")\n",
    "print(f\"   Total records: {len(final_df):,}\")\n",
    "print(f\"   Total features: {len(final_df.columns)}\")\n",
    "print(f\"   Unique locations: {final_df['location_id'].nunique()}\")\n",
    "print(f\"   Date range: {final_df['date'].min()} to {final_df['date'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Feature Categories:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"  Identity columns: {len([c for c in id_cols if c in final_df.columns])}\")\n",
    "print(f\"  Temporal columns: {len([c for c in time_cols + cyclical_cols if c in final_df.columns])}\")\n",
    "print(f\"  Pollutant columns: {len([c for c in pollutant_cols if c in final_df.columns])}\")\n",
    "print(f\"  Weather columns: {len([c for c in weather_cols if c in final_df.columns])}\")\n",
    "print(f\"  Physical feature columns: {len([c for c in physical_feature_cols if c in final_df.columns])}\")\n",
    "print(f\"  Proximity columns: {len([c for c in proximity_cols if c in final_df.columns])}\")\n",
    "print(f\"  Normalized columns: {len([c for c in normalized_cols if c in final_df.columns])}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Final Dataset Preview:\")\n",
    "print(\"-\"*50)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908890e2",
   "metadata": {},
   "source": [
    "## Step 10: Save Cleaned Dataset\n",
    "Export the final unified dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f9fa919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING FINAL CLEANED DATASET\n",
      "======================================================================\n",
      "\n",
      "âœ… Main dataset saved: data/cleaned/india_aq_cleaned_features.csv\n",
      "   Size: 106,369 records x 84 columns\n",
      "\n",
      "--------------------------------------------------\n",
      "All Columns in Final Dataset:\n",
      "--------------------------------------------------\n",
      "   1. state                               (object) - 106,369 values\n",
      "   2. district                            (object) - 106,369 values\n",
      "   3. location_id                         (int64) - 106,369 values\n",
      "   4. location_name                       (object) - 106,369 values\n",
      "   5. latitude                            (float64) - 106,369 values\n",
      "   6. longitude                           (float64) - 106,369 values\n",
      "   7. datetime_utc                        (datetime64[ns, UTC]) - 106,369 values\n",
      "   8. datetime_ist                        (datetime64[ns, Asia/Kolkata]) - 106,369 values\n",
      "   9. date                                (object) - 106,369 values\n",
      "  10. year                                (int32) - 106,369 values\n",
      "  11. month                               (int32) - 106,369 values\n",
      "  12. day                                 (int32) - 106,369 values\n",
      "  13. hour                                (int32) - 106,369 values\n",
      "  14. day_of_week                         (int32) - 106,369 values\n",
      "  15. day_name                            (object) - 106,369 values\n",
      "  16. is_weekend                          (int64) - 106,369 values\n",
      "  17. time_of_day                         (object) - 106,369 values\n",
      "  18. season                              (object) - 106,369 values\n",
      "  19. is_rush_hour                        (int64) - 106,369 values\n",
      "  20. hour_sin                            (float64) - 106,369 values\n",
      "  21. hour_cos                            (float64) - 106,369 values\n",
      "  22. day_sin                             (float64) - 106,369 values\n",
      "  23. day_cos                             (float64) - 106,369 values\n",
      "  24. month_sin                           (float64) - 106,369 values\n",
      "  25. month_cos                           (float64) - 106,369 values\n",
      "  26. pm25                                (float64) - 106,369 values\n",
      "  27. pm10                                (float64) - 106,369 values\n",
      "  28. no2                                 (float64) - 106,369 values\n",
      "  29. co                                  (float64) - 106,369 values\n",
      "  30. so2                                 (float64) - 106,369 values\n",
      "  31. o3                                  (float64) - 106,369 values\n",
      "  32. temperature                         (float64) - 106,369 values\n",
      "  33. humidity                            (float64) - 106,369 values\n",
      "  34. wind_speed                          (float64) - 106,369 values\n",
      "  35. wind_direction                      (float64) - 106,369 values\n",
      "  36. agricultural_area_sqm               (float64) - 106,369 values\n",
      "  37. agricultural_count                  (int64) - 106,369 values\n",
      "  38. agricultural_distance_m             (float64) - 33,404 values\n",
      "  39. commercial_area_sqm                 (float64) - 106,369 values\n",
      "  40. commercial_count                    (int64) - 106,369 values\n",
      "  41. commercial_distance_m               (float64) - 59,270 values\n",
      "  42. dump_sites_area_sqm                 (float64) - 106,369 values\n",
      "  43. dump_sites_count                    (int64) - 106,369 values\n",
      "  44. dump_sites_distance_m               (float64) - 40,642 values\n",
      "  45. industrial_area_sqm                 (float64) - 106,369 values\n",
      "  46. industrial_count                    (int64) - 106,369 values\n",
      "  47. industrial_distance_m               (float64) - 76,173 values\n",
      "  48. power_plants_area_sqm               (float64) - 106,369 values\n",
      "  49. power_plants_count                  (int64) - 106,369 values\n",
      "  50. power_plants_distance_m             (float64) - 19,152 values\n",
      "  51. residential_area_sqm                (float64) - 106,369 values\n",
      "  52. residential_count                   (int64) - 106,369 values\n",
      "  53. residential_distance_m              (float64) - 88,873 values\n",
      "  54. roads_count                         (int64) - 106,369 values\n",
      "  55. roads_distance_m                    (float64) - 106,369 values\n",
      "  56. roads_total_length_m                (float64) - 106,369 values\n",
      "  57. road_proximity                      (object) - 106,369 values\n",
      "  58. industrial_proximity                (object) - 106,369 values\n",
      "  59. agricultural_proximity              (object) - 106,369 values\n",
      "  60. dump_proximity                      (object) - 106,369 values\n",
      "  61. near_road                           (int64) - 106,369 values\n",
      "  62. near_industry                       (int64) - 106,369 values\n",
      "  63. near_agriculture                    (int64) - 106,369 values\n",
      "  64. near_dump                           (int64) - 106,369 values\n",
      "  65. pm25_normalized                     (float64) - 106,369 values\n",
      "  66. pm10_normalized                     (float64) - 106,369 values\n",
      "  67. no2_normalized                      (float64) - 106,369 values\n",
      "  68. co_normalized                       (float64) - 106,369 values\n",
      "  69. so2_normalized                      (float64) - 106,369 values\n",
      "  70. o3_normalized                       (float64) - 106,369 values\n",
      "  71. temperature_normalized              (float64) - 106,369 values\n",
      "  72. humidity_normalized                 (float64) - 106,369 values\n",
      "  73. wind_speed_normalized               (float64) - 106,369 values\n",
      "  74. wind_direction_normalized           (float64) - 106,369 values\n",
      "  75. pm25_scaled                         (float64) - 106,369 values\n",
      "  76. pm10_scaled                         (float64) - 106,369 values\n",
      "  77. no2_scaled                          (float64) - 106,369 values\n",
      "  78. co_scaled                           (float64) - 106,369 values\n",
      "  79. so2_scaled                          (float64) - 106,369 values\n",
      "  80. o3_scaled                           (float64) - 106,369 values\n",
      "  81. temperature_scaled                  (float64) - 106,369 values\n",
      "  82. humidity_scaled                     (float64) - 106,369 values\n",
      "  83. wind_speed_scaled                   (float64) - 106,369 values\n",
      "  84. wind_direction_scaled               (float64) - 106,369 values\n",
      "\n",
      "âœ… Statistics saved: data/cleaned/feature_statistics.csv\n",
      "\n",
      "======================================================================\n",
      "DATA CLEANING AND FEATURE ENGINEERING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ Output directory: data/cleaned\n",
      "ðŸ“ Main dataset: india_aq_cleaned_features.csv\n",
      "ðŸ“ Statistics: feature_statistics.csv\n",
      "\n",
      "ðŸŽ‰ Dataset is ready for EDA and model training!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Save Final Cleaned Dataset\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING FINAL CLEANED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"data/cleaned\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save main dataset\n",
    "output_file = f\"{output_dir}/india_aq_cleaned_features.csv\"\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ… Main dataset saved: {output_file}\")\n",
    "print(f\"   Size: {len(final_df):,} records x {len(final_df.columns)} columns\")\n",
    "\n",
    "# Save column metadata\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"All Columns in Final Dataset:\")\n",
    "print(\"-\"*50)\n",
    "for i, col in enumerate(final_df.columns, 1):\n",
    "    dtype = final_df[col].dtype\n",
    "    non_null = final_df[col].notna().sum()\n",
    "    print(f\"  {i:2d}. {col:35s} ({dtype}) - {non_null:,} values\")\n",
    "\n",
    "# Save summary statistics\n",
    "stats_file = f\"{output_dir}/feature_statistics.csv\"\n",
    "final_df.describe().to_csv(stats_file)\n",
    "print(f\"\\nâœ… Statistics saved: {stats_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA CLEANING AND FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸ“ Output directory: {output_dir}\")\n",
    "print(f\"ðŸ“ Main dataset: india_aq_cleaned_features.csv\")\n",
    "print(f\"ðŸ“ Statistics: feature_statistics.csv\")\n",
    "print(f\"\\nðŸŽ‰ Dataset is ready for EDA and model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3cfe0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Data Cleaning Operations:\n",
    "1. âœ… Removed duplicate records\n",
    "2. âœ… Removed invalid coordinates and timestamps\n",
    "3. âœ… Handled missing values (median imputation for pollutants, mean for weather)\n",
    "4. âœ… Removed outliers (>3 std deviations)\n",
    "\n",
    "### Feature Engineering:\n",
    "1. âœ… **Temporal Features**: hour, day_of_week, month, season, is_weekend, is_rush_hour\n",
    "2. âœ… **Cyclical Encoding**: hour_sin/cos, day_sin/cos, month_sin/cos\n",
    "3. âœ… **Spatial Features**: merged physical features (roads, industrial, agricultural, etc.)\n",
    "4. âœ… **Proximity Features**: categorical and binary proximity indicators\n",
    "5. âœ… **Normalized Features**: Min-Max (0-1) and Z-score scaling\n",
    "\n",
    "### Output:\n",
    "- `data/cleaned/india_aq_cleaned_features.csv` - Main cleaned dataset\n",
    "- `data/cleaned/feature_statistics.csv` - Summary statistics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
