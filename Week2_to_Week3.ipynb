{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec022946",
   "metadata": {},
   "source": [
    "WEEK-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a2fdd",
   "metadata": {},
   "source": [
    "Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2931afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c582dd2",
   "metadata": {},
   "source": [
    "import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b69b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"D:\\\\codes\\\\brain tumor project yz\\\\infosys\\\\Data_set\\\\Main_data_set\\\\FINAL_MERGED_WEATHER_OSM_NO_NULL.csv\"\n",
    "output_file = \"D:\\\\codes\\\\brain tumor project yz\\\\infosys\\\\Data_set\\\\Main_data_set\\\\FINAL_CLEANED_FEATURE_ENGINEERED_DATASET.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0c11a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Shape: (22264, 30)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(input_file)\n",
    "print(\"Original Dataset Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2096e3",
   "metadata": {},
   "source": [
    "Remove Empty / Artifact Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ac6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e1941",
   "metadata": {},
   "source": [
    "Fix Encoding Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab186475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    \"Temperature (√Ç¬∞C)\": \"Temperature (¬∞C)\",\n",
    "    \"Wind Direction (√Ç¬∞)\": \"Wind Direction (¬∞)\"\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc34e41f",
   "metadata": {},
   "source": [
    "Standardize Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e06c0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], utc=True, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa3f73",
   "metadata": {},
   "source": [
    "Remove Invalid Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de939ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"Latitude\", \"Longitude\", \"Timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c0eac8",
   "metadata": {},
   "source": [
    "Remove Duplicate Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "574dd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(\n",
    "    subset=[\"Latitude\", \"Longitude\", \"Timestamp\"],\n",
    "    inplace=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca750809",
   "metadata": {},
   "source": [
    "Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c98d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_9032\\2585188177.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mean(), inplace=True)\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_9032\\2585188177.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Weather data ‚Üí mean imputation\n",
    "weather_cols = [\n",
    "    \"Temperature (¬∞C)\", \"Humidity (%)\",\n",
    "    \"Wind Speed (m/s)\", \"Wind Direction (¬∞)\"\n",
    "]\n",
    "\n",
    "for col in weather_cols:\n",
    "    if col in df.columns:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "# OSM features ‚Üí fill with 0\n",
    "osm_cols = [\n",
    "    \"Road_Count\", \"Industrial_Count\", \"Farmland_Count\",\n",
    "    \"Dump_Site_Count\", \"Recycling_Count\", \"Green_Area_Count\"\n",
    "]\n",
    "\n",
    "for col in osm_cols:\n",
    "    if col in df.columns:\n",
    "        df[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fd81c",
   "metadata": {},
   "source": [
    "Temporal Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5650a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Hour\"] = df[\"Timestamp\"].dt.hour\n",
    "df[\"Day_of_Week\"] = df[\"Timestamp\"].dt.dayofweek\n",
    "df[\"Month\"] = df[\"Timestamp\"].dt.month\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Summer\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Monsoon\"\n",
    "    else:\n",
    "        return \"Post-Monsoon\"\n",
    "\n",
    "df[\"Season\"] = df[\"Month\"].apply(get_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5f18b",
   "metadata": {},
   "source": [
    "Normalize Pollutant & Weather Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75f46551",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_cols = [\n",
    "    \"Temperature (¬∞C)\", \"Humidity (%)\", \"Wind Speed (m/s)\",\n",
    "    \"CO AQI Value\", \"NO2 AQI Value\", \"Ozone AQI Value\",\n",
    "    \"PM2.5 AQI Value\", \"Overall AQI Value\"\n",
    "]\n",
    "\n",
    "for col in scale_cols:\n",
    "    if col in df.columns:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "        if max_val > min_val:\n",
    "            df[col + \"_Normalized\"] = (df[col] - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbff09",
   "metadata": {},
   "source": [
    "Save Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fadcab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Shape: (22264, 37)\n",
      "Preprocessing completed successfully.\n",
      "Saved as: D:\\codes\\brain tumor project yz\\infosys\\Data_set\\Main_data_set\\FINAL_CLEANED_FEATURE_ENGINEERED_DATASET.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Dataset Shape:\", df.shape)\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Preprocessing completed successfully.\")\n",
    "print(f\"Saved as: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83eb71f",
   "metadata": {},
   "source": [
    "spatial proximity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfe67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "INPUT_CSV = \"D:\\\\codes\\\\brain tumor project yz\\\\infosys\\Data_set\\\\Main_data_set\\\\FINAL_MERGED_WEATHER_OSM_NO_NULL.csv\"\n",
    "OUTPUT_CSV = \"D:\\\\codes\\\\brain tumor project yz\\\\infosys\\Data_set\\\\Main_data_set\\\\FINAL_DATASET_WITH_SPATIAL_FEATURES_COMPLETE1\"\n",
    "\n",
    "LAT_COL = \"latitude\"\n",
    "LON_COL = \"longitude\"\n",
    "\n",
    "CLUSTER_PRECISION = 1       # ~2 km clustering\n",
    "BBOX_BUFFER_DEG = 0.02       # ~2 km bbox\n",
    "DIST_THRESHOLD_KM = 2.0\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load dataset\n",
    "# -------------------------------\n",
    "print(\"üì• Loading dataset...\")\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "df = df.dropna(subset=[LAT_COL, LON_COL]).reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Total records: {len(df)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Create GeoDataFrame\n",
    "# -------------------------------\n",
    "print(\"üåç Creating GeoDataFrame...\")\n",
    "geometry = [Point(xy) for xy in zip(df[LON_COL], df[LAT_COL])]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Create LOCATION CLUSTERS\n",
    "# -------------------------------\n",
    "gdf[\"lat_round\"] = gdf[LAT_COL].round(CLUSTER_PRECISION)\n",
    "gdf[\"lon_round\"] = gdf[LON_COL].round(CLUSTER_PRECISION)\n",
    "gdf[\"location_cluster\"] = (\n",
    "    gdf[\"lat_round\"].astype(str) + \"_\" + gdf[\"lon_round\"].astype(str)\n",
    ")\n",
    "\n",
    "clusters = gdf[\"location_cluster\"].unique()\n",
    "print(f\"üìç Total location clusters: {len(clusters)}\")\n",
    "\n",
    "# Prepare result columns\n",
    "gdf[\"dist_road_km\"] = np.nan\n",
    "gdf[\"dist_industry_km\"] = np.nan\n",
    "gdf[\"dist_dump_km\"] = np.nan\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Process each cluster\n",
    "# -------------------------------\n",
    "for i, cluster in enumerate(clusters, 1):\n",
    "\n",
    "    cluster_df = gdf[gdf[\"location_cluster\"] == cluster]\n",
    "\n",
    "    lat = cluster_df[LAT_COL].mean()\n",
    "    lon = cluster_df[LON_COL].mean()\n",
    "\n",
    "    north = lat + BBOX_BUFFER_DEG\n",
    "    south = lat - BBOX_BUFFER_DEG\n",
    "    east  = lon + BBOX_BUFFER_DEG\n",
    "    west  = lon - BBOX_BUFFER_DEG\n",
    "\n",
    "    print(f\"\\nüöÄ [{i}/{len(clusters)}] Processing cluster: {cluster}\")\n",
    "    print(f\"üì¶ BBOX: {north:.4f}, {south:.4f}, {east:.4f}, {west:.4f}\")\n",
    "\n",
    "    try:\n",
    "        # -------------------------------\n",
    "        # Fetch OSM features (ONCE)\n",
    "        # -------------------------------\n",
    "        print(\"üõ£ Downloading major roads...\")\n",
    "        roads = ox.features_from_bbox(\n",
    "            bbox=(north, south, east, west),\n",
    "            tags={\"highway\": [\"motorway\", \"trunk\", \"primary\", \"secondary\"]}\n",
    "        ).to_crs(epsg=3857)\n",
    "\n",
    "        print(\"üè≠ Downloading industrial areas...\")\n",
    "        industries = ox.features_from_bbox(\n",
    "            bbox=(north, south, east, west),\n",
    "            tags={\"landuse\": \"industrial\"}\n",
    "        ).to_crs(epsg=3857)\n",
    "\n",
    "        print(\"üóë Downloading dump / landfill sites...\")\n",
    "        dumps = ox.features_from_bbox(\n",
    "            bbox=(north, south, east, west),\n",
    "            tags={\"landuse\": \"landfill\"}\n",
    "        ).to_crs(epsg=3857)\n",
    "\n",
    "        # -------------------------------\n",
    "        # Distance calculations\n",
    "        # -------------------------------\n",
    "        pts = cluster_df.copy()\n",
    "\n",
    "        if not roads.empty:\n",
    "            pts = gpd.sjoin_nearest(\n",
    "                pts, roads[[\"geometry\"]],\n",
    "                how=\"left\", distance_col=\"dist_road_m\"\n",
    "            )\n",
    "            gdf.loc[pts.index, \"dist_road_km\"] = pts[\"dist_road_m\"] / 1000\n",
    "\n",
    "        if not industries.empty:\n",
    "            pts = gpd.sjoin_nearest(\n",
    "                pts, industries[[\"geometry\"]],\n",
    "                how=\"left\", distance_col=\"dist_industry_m\"\n",
    "            )\n",
    "            gdf.loc[pts.index, \"dist_industry_km\"] = pts[\"dist_industry_m\"] / 1000\n",
    "\n",
    "        if not dumps.empty:\n",
    "            pts = gpd.sjoin_nearest(\n",
    "                pts, dumps[[\"geometry\"]],\n",
    "                how=\"left\", distance_col=\"dist_dump_m\"\n",
    "            )\n",
    "            gdf.loc[pts.index, \"dist_dump_km\"] = pts[\"dist_dump_m\"] / 1000\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping cluster {cluster} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Binary proximity flags\n",
    "# -------------------------------\n",
    "print(\"\\nüßÆ Creating binary 2km proximity flags...\")\n",
    "\n",
    "gdf[\"near_road_2km\"] = (gdf[\"dist_road_km\"] <= DIST_THRESHOLD_KM).astype(int)\n",
    "gdf[\"near_industry_2km\"] = (gdf[\"dist_industry_km\"] <= DIST_THRESHOLD_KM).astype(int)\n",
    "gdf[\"near_dump_2km\"] = (gdf[\"dist_dump_km\"] <= DIST_THRESHOLD_KM).astype(int)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Save final dataset\n",
    "# -------------------------------\n",
    "final_df = gdf.drop(\n",
    "    columns=[\"geometry\", \"lat_round\", \"lon_round\", \"location_cluster\"],\n",
    "    errors=\"ignore\"\n",
    ")\n",
    "\n",
    "final_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ FINAL DATASET SAVED SUCCESSFULLY\")\n",
    "print(f\"üìÅ Output file: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47dbfdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading dataset...\n",
      "‚úÖ Total rows: 22264\n",
      "üèô Unique cities (RAW): 22264\n",
      "üßπ Unique cities (CLEANED): 22264\n",
      "\n",
      "üîÅ Top 15 most duplicated cities:\n",
      "city_clean\n",
      "gursahaiganj            1\n",
      "dar es salaam           1\n",
      "puurs                   1\n",
      "praskoveya              1\n",
      "post falls              1\n",
      "radovis                 1\n",
      "gyanpur                 1\n",
      "puttlingen              1\n",
      "viterbo                 1\n",
      "tonala                  1\n",
      "tres pontas             1\n",
      "villa de cura           1\n",
      "vitoria da conquista    1\n",
      "reston                  1\n",
      "sand springs            1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['city'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 57\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     49\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity_clean\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m     52\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 6. Save cleaned preview (optional)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcity_clean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCITY_CLEAN_MAPPING.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ City duplicate analysis completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÑ Saved: CITY_CLEAN_MAPPING.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\codes\\brain tumor project yz\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32md:\\codes\\brain tumor project yz\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32md:\\codes\\brain tumor project yz\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['city'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load dataset\n",
    "# -------------------------------\n",
    "print(\"üì• Loading dataset...\")\n",
    "df = pd.read_csv(\"D:\\\\codes\\\\brain tumor project yz\\\\infosys\\Data_set\\\\Main_data_set\\\\FINAL_MERGED_WEATHER_OSM_NO_NULL.csv\")\n",
    "\n",
    "print(f\"‚úÖ Total rows: {len(df)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Check required columns\n",
    "# -------------------------------\n",
    "CITY_COL = \"City\"        # üîÅ change if column name differs\n",
    "LAT_COL = \"Latitude\"\n",
    "LON_COL = \"Longitude\"\n",
    "\n",
    "required_cols = [CITY_COL, LAT_COL, LON_COL]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "\n",
    "if missing:\n",
    "    raise ValueError(f\"‚ùå Missing columns: {missing}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Basic city stats (RAW)\n",
    "# -------------------------------\n",
    "raw_unique_cities = df[CITY_COL].nunique()\n",
    "print(f\"üèô Unique cities (RAW): {raw_unique_cities}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Clean city names\n",
    "# -------------------------------\n",
    "df[\"city_clean\"] = (\n",
    "    df[CITY_COL]\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"[^a-z\\s]\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "clean_unique_cities = df[\"city_clean\"].nunique()\n",
    "print(f\"üßπ Unique cities (CLEANED): {clean_unique_cities}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Top duplicate cities\n",
    "# -------------------------------\n",
    "print(\"\\nüîÅ Top 15 most duplicated cities:\")\n",
    "print(\n",
    "    df[\"city_clean\"]\n",
    "    .value_counts()\n",
    "    .head(15)\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Save cleaned preview (optional)\n",
    "# -------------------------------\n",
    "df[[\"city\", \"city_clean\"]].drop_duplicates().to_csv(\n",
    "    \"CITY_CLEAN_MAPPING.csv\", index=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ City duplicate analysis completed\")\n",
    "print(\"üìÑ Saved: CITY_CLEAN_MAPPING.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa3bcc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "United States of America    2856\n",
      "India                       2400\n",
      "Brazil                      1543\n",
      "Germany                     1281\n",
      "Russian Federation          1172\n",
      "                            ... \n",
      "Lebanon                        1\n",
      "Seychelles                     1\n",
      "State of Palestine             1\n",
      "Saint Kitts and Nevis          1\n",
      "Monaco                         1\n",
      "Name: count, Length: 176, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:\\\\codes\\\\brain tumor project yz\\\\infosys\\Data_set\\\\Main_data_set\\\\FINAL_MERGED_WEATHER_OSM_NO_NULL.csv\")\n",
    "print(df[\"Country\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d3718",
   "metadata": {},
   "source": [
    "Module 3 ‚Äì Source Labeling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc571daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded: (22264, 33)\n",
      "üìä Thresholds:\n",
      "NO2_HIGH: 4.0\n",
      "PM_HIGH: 78.0\n",
      "CO_HIGH: 1.0\n",
      "‚úÖ Source labeling completed\n",
      "\n",
      "üìå Pollution Source Distribution:\n",
      "pollution_source\n",
      "Natural       13129\n",
      "Vehicular      5671\n",
      "Industrial     3464\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìÅ Saved: FINAL_LABELED_DATASET_MODULE3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD DATA\n",
    "# -----------------------------\n",
    "INPUT_CSV = \"D:\\\\codes\\\\brain tumor project yz\\\\infosys\\\\FINAL_DATASET_WITH_SPATIAL_FEATURES_COMPLETE1.csv\"\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "print(\"‚úÖ Dataset loaded:\", df.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# CLEAN COLUMN NAMES\n",
    "# -----------------------------\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# -----------------------------\n",
    "# DEFINE THRESHOLDS (DATA-DRIVEN)\n",
    "# -----------------------------\n",
    "NO2_HIGH = df[\"no2 aqi value\"].quantile(0.75)\n",
    "PM_HIGH  = df[\"pm2.5 aqi value\"].quantile(0.75)\n",
    "CO_HIGH  = df[\"co aqi value\"].quantile(0.75)\n",
    "\n",
    "print(\"üìä Thresholds:\")\n",
    "print(\"NO2_HIGH:\", NO2_HIGH)\n",
    "print(\"PM_HIGH:\", PM_HIGH)\n",
    "print(\"CO_HIGH:\", CO_HIGH)\n",
    "\n",
    "# -----------------------------\n",
    "# SOURCE LABELING LOGIC\n",
    "# -----------------------------\n",
    "def label_source(row):\n",
    "\n",
    "    # Vehicular Pollution\n",
    "    if row[\"near_road_2km\"] == 1 and row[\"no2 aqi value\"] >= NO2_HIGH:\n",
    "        return \"Vehicular\"\n",
    "\n",
    "    # Industrial Pollution\n",
    "    if row[\"near_industry_2km\"] == 1 and row[\"pm2.5 aqi value\"] >= PM_HIGH:\n",
    "        return \"Industrial\"\n",
    "\n",
    "    # Waste / Dump related\n",
    "    if row[\"near_dump_2km\"] == 1 and row[\"pm2.5 aqi value\"] >= PM_HIGH:\n",
    "        return \"Waste Burning\"\n",
    "\n",
    "    # Agricultural / Dust\n",
    "    if row[\"pm2.5 aqi value\"] >= PM_HIGH and row[\"wind speed (m/s)\"] < 2:\n",
    "        return \"Agricultural/Dust\"\n",
    "\n",
    "    # Natural / Background\n",
    "    return \"Natural\"\n",
    "\n",
    "# -----------------------------\n",
    "# APPLY LABELING\n",
    "# -----------------------------\n",
    "df[\"pollution_source\"] = df.apply(label_source, axis=1)\n",
    "\n",
    "print(\"‚úÖ Source labeling completed\")\n",
    "\n",
    "# -----------------------------\n",
    "# CHECK DISTRIBUTION\n",
    "# -----------------------------\n",
    "print(\"\\nüìå Pollution Source Distribution:\")\n",
    "print(df[\"pollution_source\"].value_counts())\n",
    "\n",
    "# -----------------------------\n",
    "# SAVE FINAL DATASET\n",
    "# -----------------------------\n",
    "OUTPUT_CSV = \"FINAL_LABELED_DATASET_MODULE3.csv\"\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(\"\\nüìÅ Saved:\", OUTPUT_CSV)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
